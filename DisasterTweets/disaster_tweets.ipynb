{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# disaster_response_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains over 11,000 tweets associated with disaster keywords like “crash”, “quarantine”, and “bush fires” as well as the location and keyword itself.\n",
    "\n",
    "Then the text were manually classified whether the tweet referred to a disaster event or not (a joke with the word or a movie review or something non-disastrous).\n",
    "\n",
    "The data structure were inherited from Disasters on social media (https://www.figure-eight.com/data-for-everyone/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "pd.options.display.min_rows=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thunderstorm            93\n",
       "flattened               88\n",
       "stretcher               86\n",
       "mass%20murder           86\n",
       "drowning                83\n",
       "drown                   83\n",
       "sirens                  83\n",
       "engulfed                82\n",
       "obliterate              80\n",
       "fear                    80\n",
       "derailment              79\n",
       "collision               77\n",
       "electrocute             77\n",
       "deaths                  76\n",
       "hostage                 76\n",
       "derailed                76\n",
       "deluge                  76\n",
       "fatalities              74\n",
       "sunk                    74\n",
       "traumatised             74\n",
       "airplane%20accident     74\n",
       "attack                  74\n",
       "damage                  72\n",
       "destroy                 72\n",
       "inundation              72\n",
       "inundated               71\n",
       "death                   71\n",
       "crash                   71\n",
       "body%20bag              71\n",
       "bush%20fires            70\n",
       "                        ..\n",
       "natural%20disaster      30\n",
       "structural%20failure    29\n",
       "suicide%20bomber        29\n",
       "fatal                   29\n",
       "blaze                   28\n",
       "thunder                 27\n",
       "evacuated               26\n",
       "fire%20truck            26\n",
       "devastated              26\n",
       "armageddon              26\n",
       "lava                    25\n",
       "whirlwind               25\n",
       "catastrophe             25\n",
       "emergency%20plan        25\n",
       "flames                  24\n",
       "massacre                24\n",
       "exploded                23\n",
       "cyclone                 23\n",
       "typhoon                 23\n",
       "explode                 21\n",
       "desolation              20\n",
       "arsonist                20\n",
       "twister                 18\n",
       "obliteration            17\n",
       "blown%20up              17\n",
       "electrocuted            16\n",
       "rainstorm               11\n",
       "deluged                 10\n",
       "siren                   10\n",
       "tsunami                  6\n",
       "Name: keyword, Length: 219, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/tweets.csv')\n",
    "\n",
    "df.set_index('id',inplace=True)\n",
    "df.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arsonist sets cars ablaze at dealership https://t.co/0gL7NUCPlb https://t.co/u1CcBhOWh9'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Data\n",
    "# text preprocessing\n",
    "df['text'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text']=df['text'].str.strip()\n",
    "df['text']=df['text'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the Contractions\n",
    "# To expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have.\n",
    "## !pip install contractions\n",
    "\n",
    "# I was not able to install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Noises:\n",
    "Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters (symbols, emojis, and other graphic characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arsonist sets cars ablaze at dealership https://t.co/0gl7nucplb https://t.co/u1ccbhowh9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Removing url\n",
    "#df['text'].str.replace(r'\\bhttp://.*\\b')\n",
    "display(df['text'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://.*\\b','',text)\n",
    "df['text']=df['text'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, \"\", text)\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable]) \n",
    "df['text']=df['text'].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arsonist sets cars ablaze at dealership '"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rengoku sets my heart ablaze ps i missed this style of coloring i do so here it is c  '"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some tweets has emojies. They should be eliminated from the text.\n",
    "# e.g.\n",
    "df['text'].loc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function revomes the emojis from text. 'apply' function should be used for each text.\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rengoku sets my heart ablaze ps i missed this style of coloring i do so here it is c  '"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].loc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i feel directly attacked  i consider moonbin  jinjin as my bias and im currently wrecked by rocky i hate this'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].loc[11366]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0        communal violence in bhainsa telangana stones ...\n",
       "1        telangana section 144 has been imposed in bhai...\n",
       "2                 arsonist sets cars ablaze at dealership \n",
       "3                 arsonist sets cars ablaze at dealership \n",
       "4        lord jesus your love brings freedom and pardon...\n",
       "5        if this child was chinese this tweet would hav...\n",
       "6        several houses have been set ablaze in ngemsib...\n",
       "7        asansol a bjp office in salanpur village was s...\n",
       "8        national security minister kan dapaahs side ch...\n",
       "9        this creature whos soul is no longer clarent b...\n",
       "10       images showing the havoc caused by the cameroo...\n",
       "11       social media went bananas after chuba hubbard ...\n",
       "12       hausa youths set area office of apapaiganmu lo...\n",
       "13       under mamatabanerjee political violence  vanda...\n",
       "14                   amen set the whole system ablaze man \n",
       "15       images showing the havoc caused by the cameroo...\n",
       "16       no cows today but our local factory is sadly s...\n",
       "17       rengoku sets my heart ablaze ps i missed this ...\n",
       "18       paulzizkaphoto rundle ablaze wishing you all a...\n",
       "19       french cameroun set houses ablaze in ndu and r...\n",
       "20       cameroons bir soldiers on the 05012020 invaded...\n",
       "21       as fires ablaze throughout the landas the prop...\n",
       "22       thankfultuesday isaiah 432 when you pass throu...\n",
       "23       when you walk through the fire you will not be...\n",
       "24       originally they were intended to be fired at b...\n",
       "25       warm greetings to all on the occasion of lohri...\n",
       "26       another arson in njikomboyonwr the ambazombies...\n",
       "27       another public market in haiti mysteriously se...\n",
       "28                                 that is kind true sadly\n",
       "29              i swear that jam will set the world ablaze\n",
       "                               ...                        \n",
       "11340    my first listen was also in the whip i damn ne...\n",
       "11341    hes not a journalist and doesnt pretend to be ...\n",
       "11342    by all means give up sugar and carbs your body...\n",
       "11343    chanyeol 2k19 gaming destroys me messed my min...\n",
       "11344                      hell be wrecked if that happens\n",
       "11345    hes the oxygen that pumps blood to my living h...\n",
       "11346    when youre watching clemson get wrecked and se...\n",
       "11347    why would operators put new buses on school co...\n",
       "11348    democratic propaganda wrecked libya obama just...\n",
       "11349     cc this is what i was telling you the other time\n",
       "11350    tryna think of a plot but nothing comes to min...\n",
       "11351                this was me when my car got wrecked  \n",
       "11352    since everyone is talking about chen and exo o...\n",
       "11353    is it possible to be bias wrecked by your own ...\n",
       "11354    yeah proper liverpool fans wrecked man citys b...\n",
       "11355    trump and sisi rejected foreign exploitation a...\n",
       "11356    tryna think of a plot but nothing comes to min...\n",
       "11357    admit it we are all bias wrecked by soobin today \n",
       "11358    i get wrecked too smh  my biases in skz are th...\n",
       "11359    trump and sisi rejected foreign exploitation a...\n",
       "11360    man gogo version of gucci bandana just came on...\n",
       "11361                     hell be wrecked if that happens \n",
       "11362    stell wrecked ako palagi sayo haha alabtopspot...\n",
       "11363    hes the oxygen that pumps blood to my living h...\n",
       "11364    had these guys last game n fcked them talked n...\n",
       "11365    media should have warned us well in advance th...\n",
       "11366    i feel directly attacked  i consider moonbin  ...\n",
       "11367    i feel directly attacked  i consider moonbin  ...\n",
       "11368    ok who remember outcast nd the dora au those a...\n",
       "11369        jake corway wrecked while running 14th at irp\n",
       "Name: text, Length: 11370, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "def remove_punc(text):\n",
    "    return text.translate(text.maketrans('','',string.punctuation))\n",
    "df['text']=df['text'].apply(lambda x: remove_punc(x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the Typos, slang, acronyms or informal abbreviations:\n",
    "- Replace the Unicode character with equivalent ASCII character (instead of removing)\n",
    "- Replace the entity references with their actual symbols  instead of removing as HTML tags\n",
    "- Replace the Typos, slang, acronyms or informal abbreviations - depend on different situations or main topics of the NLP such as finance or medical topics.\n",
    "- List out all the hashtags/ usernames then replace with equivalent words\n",
    "- Replace the emoticon/ emoji with equivalant word meaning such as \":)\" with \"smile\" \n",
    "- Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_clean(text):\n",
    "        \"\"\"\n",
    "            Other manual text cleaning techniques\n",
    "        \"\"\"\n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"€\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text']=df['text'].apply(lambda x : other_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword       location                                               text  \\\n",
       "id                                                                             \n",
       "0   ablaze            NaN  communal violence in bhainsa telangana stones ...   \n",
       "1   ablaze            NaN  telangana section 144 has been imposed in bhai...   \n",
       "2   ablaze  New York City           arsonist sets cars ablaze at dealership    \n",
       "4   ablaze            NaN  lord jesus your love brings freedom and pardon...   \n",
       "5   ablaze             OC  if this child was chinese this tweet would hav...   \n",
       "\n",
       "    target                                          tokenized  \n",
       "id                                                             \n",
       "0        1  [communal, violence, in, bhainsa, telangana, s...  \n",
       "1        1  [telangana, section, 144, has, been, imposed, ...  \n",
       "2        1     [arsonist, sets, cars, ablaze, at, dealership]  \n",
       "4        0  [lord, jesus, your, love, brings, freedom, and...  \n",
       "5        0  [if, this, child, was, chinese, this, tweet, w...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing duplicate rows\n",
    "df.drop_duplicates(subset=['text'], keep='first',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "you can use textblob.TextBlob to correct the typo. But it should be used very carefully as it might change the meaning of the text.\n",
    "\n",
    "From textblob import TextBlob\n",
    "\n",
    "print(\"Test: \", TextBlob(\"sleapy and tehre is no plaxe I'm gioong to.\").correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword       location                                               text  \\\n",
       "id                                                                             \n",
       "0   ablaze            NaN  communal violence in bhainsa telangana stones ...   \n",
       "1   ablaze            NaN  telangana section 144 has been imposed in bhai...   \n",
       "2   ablaze  New York City           arsonist sets cars ablaze at dealership    \n",
       "4   ablaze            NaN  lord jesus your love brings freedom and pardon...   \n",
       "5   ablaze             OC  if this child was chinese this tweet would hav...   \n",
       "\n",
       "    target                                          tokenized  \n",
       "id                                                             \n",
       "0        1  [communal, violence, in, bhainsa, telangana, s...  \n",
       "1        1  [telangana, section, 144, has, been, imposed, ...  \n",
       "2        1     [arsonist, sets, cars, ablaze, at, dealership]  \n",
       "4        0  [lord, jesus, your, love, brings, freedom, and...  \n",
       "5        0  [if, this, child, was, chinese, this, tweet, w...  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tekenization:\n",
    "from nltk import word_tokenize\n",
    "df['tokenized']=df['text'].apply(word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "      <td>[communal, violence, bhainsa, telangana, stone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "      <td>[telangana, section, 144, imposed, bhainsa, ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, dealership]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "      <td>[lord, jesus, love, brings, freedom, pardon, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "      <td>[child, chinese, tweet, would, gone, viral, so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword       location                                               text  \\\n",
       "id                                                                             \n",
       "0   ablaze            NaN  communal violence in bhainsa telangana stones ...   \n",
       "1   ablaze            NaN  telangana section 144 has been imposed in bhai...   \n",
       "2   ablaze  New York City           arsonist sets cars ablaze at dealership    \n",
       "4   ablaze            NaN  lord jesus your love brings freedom and pardon...   \n",
       "5   ablaze             OC  if this child was chinese this tweet would hav...   \n",
       "\n",
       "    target                                          tokenized  \\\n",
       "id                                                              \n",
       "0        1  [communal, violence, in, bhainsa, telangana, s...   \n",
       "1        1  [telangana, section, 144, has, been, imposed, ...   \n",
       "2        1     [arsonist, sets, cars, ablaze, at, dealership]   \n",
       "4        0  [lord, jesus, your, love, brings, freedom, and...   \n",
       "5        0  [if, this, child, was, chinese, this, tweet, w...   \n",
       "\n",
       "                                    stopwords_removed  \n",
       "id                                                     \n",
       "0   [communal, violence, bhainsa, telangana, stone...  \n",
       "1   [telangana, section, 144, imposed, bhainsa, ja...  \n",
       "2          [arsonist, sets, cars, ablaze, dealership]  \n",
       "4   [lord, jesus, love, brings, freedom, pardon, f...  \n",
       "5   [child, chinese, tweet, would, gone, viral, so...  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words:\n",
    "from nltk.corpus import stopwords\n",
    "#stop=stopwords.words('English')\n",
    "# df['stop_words_removed']=df['tokenized'].apply(lambda x : [word for word in x if not in stops])\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "df['stopwords_removed'] = df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.821932\n",
       "1    0.178068\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have unbalanced sample. only 17.8% of the comments is related to the disasters.\n",
    "df['target'].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X,y\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, y_train, y_test= train_test_split(df['text'],df['target'],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<8200x18286 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 121848 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train_vect = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train_vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 18286\n",
      "Every 2000th feature:\n",
      "['00', 'becaus', 'cosmopolitan', 'extinct', 'hurst', 'make', 'pauli', 'runners', 'tamara', 'write']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\n",
    "\n",
    "# As can be seen in below, 18268 features are a lot. we should find a way to decrease the number of features. \n",
    "# But before let see how is the performance of our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation AUC: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Lets check LogisticRegression on this data set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_train_vect, y_train, cv=5,scoring='roc_auc')\n",
    "\n",
    "print(\"Mean cross-validation AUC: {:.2f}\".format(np.mean(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets decrese the number of features with lemmatization:\n",
    "#      Do not run this code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "df['text_lemmatized'] = df['text'].apply(lemmatize_text)\n",
    "#vect = CountVectorizer().fit(text_train)\n",
    "#X_train = vect.transform(text_train)\n",
    "df['text_lemmatized'].iloc[:20]\n",
    "\n",
    "#print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number iof features in countVectorizor:  3171\n",
      "CountVectorizer AUC:  0.767954623352087\n",
      "CountVectorizer f1:  0.6625766871165644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "print('The number iof features in countVectorizor: ',len(vect.get_feature_names()))\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "\n",
    "print('CountVectorizer AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('CountVectorizer f1: ', f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features in tf-idf:  3171\n",
      "tf-idf AUC:  0.6839081580807531\n",
      "tf-idf f1:  0.527536231884058\n"
     ]
    }
   ],
   "source": [
    "# Lets use tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "# X_train=df['text_lemmatized']\n",
    "vect = TfidfVectorizer(min_df=5).fit(text_train)\n",
    "print('The number of features in tf-idf: ',len(vect.get_feature_names()))\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "\n",
    "print('tf-idf AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('tf-idf f1: ', f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number iof features in countVectorizor:  5470\n",
      "CountVectorizer AUC:  0.7565863110249129\n",
      "tf-idf f1:  0.6427688504326329\n"
     ]
    }
   ],
   "source": [
    "# n-gram- count\n",
    "# lets try to see if the combination of words can improve the performance of the model\n",
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(text_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "print('The number iof features in countVectorizor: ',len(vect.get_feature_names()))\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "print('CountVectorizer AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('tf-idf f1: ', f1_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number iof features in TfidfVectorizor:  5470\n",
      "tf-idf AUC:  0.6813254640350217\n",
      "tf-idf f1:  0.526002971768202\n"
     ]
    }
   ],
   "source": [
    "# n-gram- tf-idf\n",
    "# lets try to see if the combination of words can improve the performance of the model\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(text_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "print('The number iof features in TfidfVectorizor: ',len(vect.get_feature_names()))\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "print('tf-idf AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('tf-idf f1: ', f1_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('clf',\n",
       "                                        LogisticRegression(C=1.0,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'clf__C': [0.01, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try to improve the performance by tunning hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe=Pipeline([('clf',LogisticRegression())])\n",
    "param_grid = {'clf__C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
