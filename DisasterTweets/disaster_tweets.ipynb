{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# disaster_response_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains over 11,000 tweets associated with disaster keywords like “crash”, “quarantine”, and “bush fires” as well as the location and keyword itself.\n",
    "\n",
    "Then the text were manually classified whether the tweet referred to a disaster event or not (a joke with the word or a movie review or something non-disastrous).\n",
    "\n",
    "The data structure were inherited from Disasters on social media (https://www.figure-eight.com/data-for-everyone/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b56524a9659c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "pd.options.display.min_rows=100\n",
    "pd.options.display.width=75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/tweets.csv')\n",
    "\n",
    "df.set_index('id',inplace=True)\n",
    "df.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data\n",
    "# text preprocessing\n",
    "df['text'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text']=df['text'].str.strip()\n",
    "df['text']=df['text'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the Contractions\n",
    "# To expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have.\n",
    "## !pip install contractions\n",
    "\n",
    "# I was not able to install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Noises:\n",
    "Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters (symbols, emojis, and other graphic characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing url\n",
    "#df['text'].str.replace(r'\\bhttp://.*\\b')\n",
    "display(df['text'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2e05be5d2857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'https?://.*\\b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://.*\\b','',text)\n",
    "df['text']=df['text'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-06c86ae1f0ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mremove_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, \"\", text)\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable]) \n",
    "df['text']=df['text'].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arsonist sets cars ablaze at dealership'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rengoku sets my heart ablaze p.s. i missed this style of coloring i do so here it is c: #'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some tweets has emojies. They should be eliminated from the text.\n",
    "# e.g.\n",
    "df['text'].loc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function revomes the emojis from text. 'apply' function should be used for each text.\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rengoku sets my heart ablaze p.s. i missed this style of coloring i do so here it is c: #'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].loc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i feel directly attacked  i consider moonbin  jinjin as my bias and im currently wrecked by rocky i hate this'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].loc[11366]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0        communal violence in bhainsa telangana stones ...\n",
       "1        telangana section 144 has been imposed in bhai...\n",
       "2                  arsonist sets cars ablaze at dealership\n",
       "3                  arsonist sets cars ablaze at dealership\n",
       "4        lord jesus your love brings freedom and pardon...\n",
       "5        if this child was chinese this tweet would hav...\n",
       "6        several houses have been set ablaze in ngemsib...\n",
       "7        asansol a bjp office in salanpur village was s...\n",
       "8        national security minister kan dapaahs side ch...\n",
       "9        this creature whos soul is no longer clarent b...\n",
       "10       images showing the havoc caused by the cameroo...\n",
       "11       social media went bananas after chuba hubbard ...\n",
       "12       hausa youths set area office of apapaiganmu lo...\n",
       "13       under mamatabanerjee political violence  vanda...\n",
       "14                    amen set the whole system ablaze man\n",
       "15       images showing the havoc caused by the cameroo...\n",
       "16       no cows today but our local factory is sadly s...\n",
       "17       rengoku sets my heart ablaze ps i missed this ...\n",
       "18       paulzizkaphoto rundle ablaze wishing you all a...\n",
       "19       french cameroun set houses ablaze in ndu and r...\n",
       "20       cameroons bir soldiers on the 05012020 invaded...\n",
       "21       as fires ablaze throughout the landas the prop...\n",
       "22       thankfultuesday isaiah 432 when you pass throu...\n",
       "23       when you walk through the fire you will not be...\n",
       "24       originally they were intended to be fired at b...\n",
       "25       warm greetings to all on the occasion of lohri...\n",
       "26       another arson in njikomboyonwr the ambazombies...\n",
       "27       another public market in haiti mysteriously se...\n",
       "28                                 that is kind true sadly\n",
       "29              i swear that jam will set the world ablaze\n",
       "                               ...                        \n",
       "11340    my first listen was also in the whip i damn ne...\n",
       "11341    hes not a journalist and doesnt pretend to be ...\n",
       "11342    by all means give up sugar and carbs your body...\n",
       "11343    chanyeol 2k19 gaming destroys me messed my min...\n",
       "11344                      hell be wrecked if that happens\n",
       "11345    hes the oxygen that pumps blood to my living h...\n",
       "11346    when youre watching clemson get wrecked and se...\n",
       "11347    why would operators put new buses on school co...\n",
       "11348    democratic propaganda wrecked libya obama just...\n",
       "11349     cc this is what i was telling you the other time\n",
       "11350    tryna think of a plot but nothing comes to min...\n",
       "11351                 this was me when my car got wrecked \n",
       "11352    since everyone is talking about chen and exo o...\n",
       "11353    is it possible to be bias wrecked by your own ...\n",
       "11354    yeah proper liverpool fans wrecked man citys b...\n",
       "11355    trump and sisi rejected foreign exploitation a...\n",
       "11356    tryna think of a plot but nothing comes to min...\n",
       "11357     admit it we are all bias wrecked by soobin today\n",
       "11358    i get wrecked too smh  my biases in skz are th...\n",
       "11359    trump and sisi rejected foreign exploitation a...\n",
       "11360    man gogo version of gucci bandana just came on...\n",
       "11361                      hell be wrecked if that happens\n",
       "11362    stell wrecked ako palagi sayo haha alabtopspot...\n",
       "11363    hes the oxygen that pumps blood to my living h...\n",
       "11364    had these guys last game n fcked them talked n...\n",
       "11365    media should have warned us well in advance th...\n",
       "11366    i feel directly attacked  i consider moonbin  ...\n",
       "11367    i feel directly attacked  i consider moonbin  ...\n",
       "11368    ok who remember outcast nd the dora au those a...\n",
       "11369        jake corway wrecked while running 14th at irp\n",
       "Name: text, Length: 11370, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "def remove_punc(text):\n",
    "    return text.translate(text.maketrans('','',string.punctuation))\n",
    "df['text']=df['text'].apply(lambda x: remove_punc(x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the Typos, slang, acronyms or informal abbreviations:\n",
    "- Replace the Unicode character with equivalent ASCII character (instead of removing)\n",
    "- Replace the entity references with their actual symbols  instead of removing as HTML tags\n",
    "- Replace the Typos, slang, acronyms or informal abbreviations - depend on different situations or main topics of the NLP such as finance or medical topics.\n",
    "- List out all the hashtags/ usernames then replace with equivalent words\n",
    "- Replace the emoticon/ emoji with equivalant word meaning such as \":)\" with \"smile\" \n",
    "- Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_clean(text):\n",
    "        \n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"€\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['text']=df['text'].apply(lambda x : other_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword       location  \\\n",
       "id                          \n",
       "0   ablaze            NaN   \n",
       "1   ablaze            NaN   \n",
       "2   ablaze  New York City   \n",
       "4   ablaze            NaN   \n",
       "5   ablaze             OC   \n",
       "\n",
       "                                                 text  target  \n",
       "id                                                             \n",
       "0   communal violence in bhainsa telangana stones ...       1  \n",
       "1   telangana section 144 has been imposed in bhai...       1  \n",
       "2             arsonist sets cars ablaze at dealership       1  \n",
       "4   lord jesus your love brings freedom and pardon...       0  \n",
       "5   if this child was chinese this tweet would hav...       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing duplicate rows\n",
    "df.drop_duplicates(subset=['text'], keep='first',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction\n",
    "you can use textblob.TextBlob to correct the typo. But it should be used very carefully as it might change the meaning of the text.\n",
    "\n",
    "From textblob import TextBlob\n",
    "\n",
    "e.g.:\n",
    "\n",
    "print(\"Test: \", TextBlob(\"sleapy and tehre is no plaxe I'm gioong to.\").correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword       location  \\\n",
       "id                          \n",
       "0   ablaze            NaN   \n",
       "1   ablaze            NaN   \n",
       "2   ablaze  New York City   \n",
       "4   ablaze            NaN   \n",
       "5   ablaze             OC   \n",
       "\n",
       "                                                 text  target  \\\n",
       "id                                                              \n",
       "0   communal violence in bhainsa telangana stones ...       1   \n",
       "1   telangana section 144 has been imposed in bhai...       1   \n",
       "2             arsonist sets cars ablaze at dealership       1   \n",
       "4   lord jesus your love brings freedom and pardon...       0   \n",
       "5   if this child was chinese this tweet would hav...       0   \n",
       "\n",
       "                                            tokenized  \n",
       "id                                                     \n",
       "0   [communal, violence, in, bhainsa, telangana, s...  \n",
       "1   [telangana, section, 144, has, been, imposed, ...  \n",
       "2      [arsonist, sets, cars, ablaze, at, dealership]  \n",
       "4   [lord, jesus, your, love, brings, freedom, and...  \n",
       "5   [if, this, child, was, chinese, this, tweet, w...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tekenization:\n",
    "from nltk import word_tokenize\n",
    "df['tokenized']=df['text'].apply(word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword       location  \\\n",
       "id                          \n",
       "0   ablaze            NaN   \n",
       "1   ablaze            NaN   \n",
       "2   ablaze  New York City   \n",
       "4   ablaze            NaN   \n",
       "5   ablaze             OC   \n",
       "\n",
       "                                                 text  target  \\\n",
       "id                                                              \n",
       "0   communal violence in bhainsa telangana stones ...       1   \n",
       "1   telangana section 144 has been imposed in bhai...       1   \n",
       "2             arsonist sets cars ablaze at dealership       1   \n",
       "4   lord jesus your love brings freedom and pardon...       0   \n",
       "5   if this child was chinese this tweet would hav...       0   \n",
       "\n",
       "                                            tokenized  \n",
       "id                                                     \n",
       "0   [communal, violence, in, bhainsa, telangana, s...  \n",
       "1   [telangana, section, 144, has, been, imposed, ...  \n",
       "2      [arsonist, sets, cars, ablaze, at, dealership]  \n",
       "4   [lord, jesus, your, love, brings, freedom, and...  \n",
       "5   [if, this, child, was, chinese, this, tweet, w...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words:\n",
    "#from nltk.corpus import stopwords\n",
    "#stop=stopwords.words('English')\n",
    "# df['stop_words_removed']=df['tokenized'].apply(lambda x : [word for word in x if not in stops])\n",
    "\n",
    "# stop = set(stopwords.words('english'))\n",
    "#df['stopwords_removed'] = df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming:\n",
    "from nltk.stem import PorterStemmer\n",
    "def porter_stemmer (text):\n",
    "    stemmer=nltk.PorterStemmer()\n",
    "    stems=[stemmer.stem(word) for word in text]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>PorterStemmer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "      <td>[commun, violenc, in, bhainsa, telangana, ston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "      <td>[telangana, section, 144, ha, been, impos, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, set, car, ablaz, at, dealership]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "      <td>[lord, jesu, your, love, bring, freedom, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "      <td>[if, thi, child, wa, chines, thi, tweet, would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, England</td>\n",
       "      <td>several houses have been set ablaze in ngemsib...</td>\n",
       "      <td>1</td>\n",
       "      <td>[several, houses, have, been, set, ablaze, in,...</td>\n",
       "      <td>[sever, hous, have, been, set, ablaz, in, ngem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Bharat</td>\n",
       "      <td>asansol a bjp office in salanpur village was s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[asansol, a, bjp, office, in, salanpur, villag...</td>\n",
       "      <td>[asansol, a, bjp, offic, in, salanpur, villag,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Accra, Ghana</td>\n",
       "      <td>national security minister kan dapaahs side ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>[national, security, minister, kan, dapaahs, s...</td>\n",
       "      <td>[nation, secur, minist, kan, dapaah, side, chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Searching</td>\n",
       "      <td>this creature whos soul is no longer clarent b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, creature, whos, soul, is, no, longer, c...</td>\n",
       "      <td>[thi, creatur, who, soul, is, no, longer, clar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images showing the havoc caused by the cameroo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[images, showing, the, havoc, caused, by, the,...</td>\n",
       "      <td>[imag, show, the, havoc, caus, by, the, camero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>social media went bananas after chuba hubbard ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[social, media, went, bananas, after, chuba, h...</td>\n",
       "      <td>[social, media, went, banana, after, chuba, hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hausa youths set area office of apapaiganmu lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hausa, youths, set, area, office, of, apapaig...</td>\n",
       "      <td>[hausa, youth, set, area, offic, of, apapaigan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>under mamatabanerjee political violence  vanda...</td>\n",
       "      <td>1</td>\n",
       "      <td>[under, mamatabanerjee, political, violence, v...</td>\n",
       "      <td>[under, mamatabanerje, polit, violenc, vandal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Reno, NV</td>\n",
       "      <td>amen set the whole system ablaze man</td>\n",
       "      <td>0</td>\n",
       "      <td>[amen, set, the, whole, system, ablaze, man]</td>\n",
       "      <td>[amen, set, the, whole, system, ablaz, man]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images showing the havoc caused by the cameroo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[images, showing, the, havoc, caused, by, the,...</td>\n",
       "      <td>[imag, show, the, havoc, caus, by, the, camero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no cows today but our local factory is sadly s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[no, cows, today, but, our, local, factory, is...</td>\n",
       "      <td>[no, cow, today, but, our, local, factori, is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rengoku sets my heart ablaze postscript i miss...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rengoku, sets, my, heart, ablaze, postscript,...</td>\n",
       "      <td>[rengoku, set, my, heart, ablaz, postscript, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>paulzizkaphoto rundle ablaze wishing you all a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[paulzizkaphoto, rundle, ablaze, wishing, you,...</td>\n",
       "      <td>[paulzizkaphoto, rundl, ablaz, wish, you, all,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>french cameroun set houses ablaze in ndu and r...</td>\n",
       "      <td>1</td>\n",
       "      <td>[french, cameroun, set, houses, ablaze, in, nd...</td>\n",
       "      <td>[french, cameroun, set, hous, ablaz, in, ndu, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cameroons bir soldiers on the 05012020 invaded...</td>\n",
       "      <td>1</td>\n",
       "      <td>[cameroons, bir, soldiers, on, the, 05012020, ...</td>\n",
       "      <td>[cameroon, bir, soldier, on, the, 05012020, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>as fires ablaze throughout the landas the prop...</td>\n",
       "      <td>1</td>\n",
       "      <td>[as, fires, ablaze, throughout, the, landas, t...</td>\n",
       "      <td>[as, fire, ablaz, throughout, the, landa, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Italy</td>\n",
       "      <td>thankfultuesday isaiah 432 when you pass throu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[thankfultuesday, isaiah, 432, when, you, pass...</td>\n",
       "      <td>[thankfultuesday, isaiah, 432, when, you, pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>` ˗ˏˋ i'm⠀waiting⠀for⠀you⠀to⠀pour⠀my⠀𝘀𝗶𝗻𝘀⠀onto...</td>\n",
       "      <td>when you walk through the fire you will not be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[when, you, walk, through, the, fire, you, wil...</td>\n",
       "      <td>[when, you, walk, through, the, fire, you, wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Okielahoma</td>\n",
       "      <td>originally they were intended to be fired at b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[originally, they, were, intended, to, be, fir...</td>\n",
       "      <td>[origin, they, were, intend, to, be, fire, at,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>warm greetings to all on the occasion of lohri...</td>\n",
       "      <td>0</td>\n",
       "      <td>[warm, greetings, to, all, on, the, occasion, ...</td>\n",
       "      <td>[warm, greet, to, all, on, the, occas, of, loh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>another arson in njikomboyonwr the ambazombies...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, arson, in, njikomboyonwr, the, ambaz...</td>\n",
       "      <td>[anoth, arson, in, njikomboyonwr, the, ambazom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Havana 3am</td>\n",
       "      <td>another public market in haiti mysteriously se...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, public, market, in, haiti, mysteriou...</td>\n",
       "      <td>[anoth, public, market, in, haiti, mysteri, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that is kind true sadly</td>\n",
       "      <td>0</td>\n",
       "      <td>[that, is, kind, true, sadly]</td>\n",
       "      <td>[that, is, kind, true, sadli]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i swear that jam will set the world ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, swear, that, jam, will, set, the, world, a...</td>\n",
       "      <td>[i, swear, that, jam, will, set, the, world, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marivan kurdistan province monday jan 13th 202...</td>\n",
       "      <td>1</td>\n",
       "      <td>[marivan, kurdistan, province, monday, jan, 13...</td>\n",
       "      <td>[marivan, kurdistan, provinc, monday, jan, 13t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11336</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>𝕸𝖔𝖆𝖗𝖒𝖞</td>\n",
       "      <td>admit it we are all bias wrecked by soobin today</td>\n",
       "      <td>0</td>\n",
       "      <td>[admit, it, we, are, all, bias, wrecked, by, s...</td>\n",
       "      <td>[admit, it, we, are, all, bia, wreck, by, soob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11337</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Malilipot, Bicol Region</td>\n",
       "      <td>hi so ive started to stan the girl group ans b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hi, so, ive, started, to, stan, the, girl, gr...</td>\n",
       "      <td>[hi, so, ive, start, to, stan, the, girl, grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11338</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Puchong</td>\n",
       "      <td>kesian ular we have wrecked their natural habitat</td>\n",
       "      <td>1</td>\n",
       "      <td>[kesian, ular, we, have, wrecked, their, natur...</td>\n",
       "      <td>[kesian, ular, we, have, wreck, their, natur, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11339</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>fan acc</td>\n",
       "      <td>my twat of an art teacher  what grades do you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, twat, of, an, art, teacher, what, grades,...</td>\n",
       "      <td>[my, twat, of, an, art, teacher, what, grade, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11340</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "      <td>my first listen was also in the whip i damn ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, first, listen, was, also, in, the, whip, ...</td>\n",
       "      <td>[my, first, listen, wa, also, in, the, whip, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11341</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hes not a journalist and doesnt pretend to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hes, not, a, journalist, and, doesnt, pretend...</td>\n",
       "      <td>[he, not, a, journalist, and, doesnt, pretend,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11342</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Shropshire</td>\n",
       "      <td>by all means give up sugar and carbs your body...</td>\n",
       "      <td>0</td>\n",
       "      <td>[by, all, means, give, up, sugar, and, carbs, ...</td>\n",
       "      <td>[by, all, mean, give, up, sugar, and, carb, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>EXO's 💜</td>\n",
       "      <td>chanyeol 2k19 gaming destroys me messed my min...</td>\n",
       "      <td>0</td>\n",
       "      <td>[chanyeol, 2k19, gaming, destroys, me, messed,...</td>\n",
       "      <td>[chanyeol, 2k19, game, destroy, me, mess, my, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11344</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>🇲🇾</td>\n",
       "      <td>hell be wrecked if that happens</td>\n",
       "      <td>0</td>\n",
       "      <td>[hell, be, wrecked, if, that, happens]</td>\n",
       "      <td>[hell, be, wreck, if, that, happen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11345</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hes the oxygen that pumps blood to my living h...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hes, the, oxygen, that, pumps, blood, to, my,...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11346</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Alabama, USA</td>\n",
       "      <td>when youre watching clemson get wrecked and se...</td>\n",
       "      <td>0</td>\n",
       "      <td>[when, youre, watching, clemson, get, wrecked,...</td>\n",
       "      <td>[when, your, watch, clemson, get, wreck, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11347</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>why would operators put new buses on school co...</td>\n",
       "      <td>0</td>\n",
       "      <td>[why, would, operators, put, new, buses, on, s...</td>\n",
       "      <td>[whi, would, oper, put, new, buse, on, school,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11348</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Capon Bridge, WV,</td>\n",
       "      <td>democratic propaganda wrecked libya obama just...</td>\n",
       "      <td>0</td>\n",
       "      <td>[democratic, propaganda, wrecked, libya, obama...</td>\n",
       "      <td>[democrat, propaganda, wreck, libya, obama, ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11349</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Zion, Nigeria</td>\n",
       "      <td>cc this is what i was telling you the other time</td>\n",
       "      <td>0</td>\n",
       "      <td>[cc, this, is, what, i, was, telling, you, the...</td>\n",
       "      <td>[cc, thi, is, what, i, wa, tell, you, the, oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11350</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tryna think of a plot but nothing comes to min...</td>\n",
       "      <td>0</td>\n",
       "      <td>[tryna, think, of, a, plot, but, nothing, come...</td>\n",
       "      <td>[tryna, think, of, a, plot, but, noth, come, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11351</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this was me when my car got wrecked</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, was, me, when, my, car, got, wrecked]</td>\n",
       "      <td>[thi, wa, me, when, my, car, got, wreck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11352</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>since everyone is talking about chen and exo o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[since, everyone, is, talking, about, chen, an...</td>\n",
       "      <td>[sinc, everyon, is, talk, about, chen, and, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11353</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>magic shop</td>\n",
       "      <td>is it possible to be bias wrecked by your own ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, it, possible, to, be, bias, wrecked, by, ...</td>\n",
       "      <td>[is, it, possibl, to, be, bia, wreck, by, your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11354</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yeah proper liverpool fans wrecked man citys b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[yeah, proper, liverpool, fans, wrecked, man, ...</td>\n",
       "      <td>[yeah, proper, liverpool, fan, wreck, man, cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11355</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Recife</td>\n",
       "      <td>trump and sisi rejected foreign exploitation a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[trump, and, sisi, rejected, foreign, exploita...</td>\n",
       "      <td>[trump, and, sisi, reject, foreign, exploit, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11358</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>gguk's pocket ♡</td>\n",
       "      <td>i get wrecked too shake my head  my biases in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, get, wrecked, too, shake, my, head, my, bi...</td>\n",
       "      <td>[i, get, wreck, too, shake, my, head, my, bias...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11359</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>trump and sisi rejected foreign exploitation a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[trump, and, sisi, rejected, foreign, exploita...</td>\n",
       "      <td>[trump, and, sisi, reject, foreign, exploit, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>D(M)V</td>\n",
       "      <td>man gogo version of gucci bandana just came on...</td>\n",
       "      <td>0</td>\n",
       "      <td>[man, gogo, version, of, gucci, bandana, just,...</td>\n",
       "      <td>[man, gogo, version, of, gucci, bandana, just,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11362</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>feuille d'érable</td>\n",
       "      <td>stell wrecked ako palagi sayo haha alabtopspot...</td>\n",
       "      <td>0</td>\n",
       "      <td>[stell, wrecked, ako, palagi, sayo, haha, alab...</td>\n",
       "      <td>[stell, wreck, ako, palagi, sayo, haha, alabto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11363</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hes the oxygen that pumps blood to my living h...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hes, the, oxygen, that, pumps, blood, to, my,...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11364</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>had these guys last game n fcked them talked n...</td>\n",
       "      <td>0</td>\n",
       "      <td>[had, these, guys, last, game, n, fcked, them,...</td>\n",
       "      <td>[had, these, guy, last, game, n, fcked, them, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Blue State in a red sea</td>\n",
       "      <td>media should have warned us well in advance th...</td>\n",
       "      <td>0</td>\n",
       "      <td>[media, should, have, warned, us, well, in, ad...</td>\n",
       "      <td>[media, should, have, warn, us, well, in, adva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>arohaonces</td>\n",
       "      <td>i feel directly attacked  i consider moonbin  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, feel, directly, attacked, i, consider, moo...</td>\n",
       "      <td>[i, feel, directli, attack, i, consid, moonbin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>auroraborealis</td>\n",
       "      <td>ok who remember outcast nd the dora au those a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ok, who, remember, outcast, nd, the, dora, au...</td>\n",
       "      <td>[ok, who, rememb, outcast, nd, the, dora, au, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11369</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jake corway wrecked while running 14th at irp</td>\n",
       "      <td>1</td>\n",
       "      <td>[jake, corway, wrecked, while, running, 14th, ...</td>\n",
       "      <td>[jake, corway, wreck, while, run, 14th, at, irp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10898 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                                           location  \\\n",
       "id                                                                  \n",
       "0       ablaze                                                NaN   \n",
       "1       ablaze                                                NaN   \n",
       "2       ablaze                                      New York City   \n",
       "4       ablaze                                                NaN   \n",
       "5       ablaze                                                 OC   \n",
       "6       ablaze                                    London, England   \n",
       "7       ablaze                                             Bharat   \n",
       "8       ablaze                                       Accra, Ghana   \n",
       "9       ablaze                                          Searching   \n",
       "10      ablaze                                                NaN   \n",
       "11      ablaze                                                NaN   \n",
       "12      ablaze                                                NaN   \n",
       "13      ablaze                                          HYDERABAD   \n",
       "14      ablaze                                           Reno, NV   \n",
       "15      ablaze                                                NaN   \n",
       "16      ablaze                                                NaN   \n",
       "17      ablaze                                                NaN   \n",
       "18      ablaze                                          Worldwide   \n",
       "19      ablaze                                                NaN   \n",
       "20      ablaze                                                NaN   \n",
       "21      ablaze                                                NaN   \n",
       "22      ablaze                                              Italy   \n",
       "23      ablaze  ` ˗ˏˋ i'm⠀waiting⠀for⠀you⠀to⠀pour⠀my⠀𝘀𝗶𝗻𝘀⠀onto...   \n",
       "24      ablaze                                         Okielahoma   \n",
       "25      ablaze                                                NaN   \n",
       "26      ablaze                                                NaN   \n",
       "27      ablaze                                         Havana 3am   \n",
       "28      ablaze                                                NaN   \n",
       "29      ablaze                                                NaN   \n",
       "30      ablaze                                                NaN   \n",
       "...        ...                                                ...   \n",
       "11336  wrecked                                             𝕸𝖔𝖆𝖗𝖒𝖞   \n",
       "11337  wrecked                            Malilipot, Bicol Region   \n",
       "11338  wrecked                                            Puchong   \n",
       "11339  wrecked                                            fan acc   \n",
       "11340  wrecked                                  Oklahoma City, OK   \n",
       "11341  wrecked                                                NaN   \n",
       "11342  wrecked                                         Shropshire   \n",
       "11343  wrecked                                            EXO's 💜   \n",
       "11344  wrecked                                                 🇲🇾   \n",
       "11345  wrecked                                                NaN   \n",
       "11346  wrecked                                       Alabama, USA   \n",
       "11347  wrecked                                                NaN   \n",
       "11348  wrecked                                  Capon Bridge, WV,   \n",
       "11349  wrecked                                      Zion, Nigeria   \n",
       "11350  wrecked                                                NaN   \n",
       "11351  wrecked                                                NaN   \n",
       "11352  wrecked                                                NaN   \n",
       "11353  wrecked                                         magic shop   \n",
       "11354  wrecked                                                NaN   \n",
       "11355  wrecked                                             Recife   \n",
       "11358  wrecked                                    gguk's pocket ♡   \n",
       "11359  wrecked                                     Washington, DC   \n",
       "11360  wrecked                                              D(M)V   \n",
       "11362  wrecked                                   feuille d'érable   \n",
       "11363  wrecked                                                NaN   \n",
       "11364  wrecked                                                NaN   \n",
       "11365  wrecked                            Blue State in a red sea   \n",
       "11366  wrecked                                         arohaonces   \n",
       "11368  wrecked                                     auroraborealis   \n",
       "11369  wrecked                                                NaN   \n",
       "\n",
       "                                                    text  target  \\\n",
       "id                                                                 \n",
       "0      communal violence in bhainsa telangana stones ...       1   \n",
       "1      telangana section 144 has been imposed in bhai...       1   \n",
       "2                arsonist sets cars ablaze at dealership       1   \n",
       "4      lord jesus your love brings freedom and pardon...       0   \n",
       "5      if this child was chinese this tweet would hav...       0   \n",
       "6      several houses have been set ablaze in ngemsib...       1   \n",
       "7      asansol a bjp office in salanpur village was s...       1   \n",
       "8      national security minister kan dapaahs side ch...       0   \n",
       "9      this creature whos soul is no longer clarent b...       0   \n",
       "10     images showing the havoc caused by the cameroo...       1   \n",
       "11     social media went bananas after chuba hubbard ...       0   \n",
       "12     hausa youths set area office of apapaiganmu lo...       1   \n",
       "13     under mamatabanerjee political violence  vanda...       1   \n",
       "14                  amen set the whole system ablaze man       0   \n",
       "15     images showing the havoc caused by the cameroo...       1   \n",
       "16     no cows today but our local factory is sadly s...       1   \n",
       "17     rengoku sets my heart ablaze postscript i miss...       0   \n",
       "18     paulzizkaphoto rundle ablaze wishing you all a...       0   \n",
       "19     french cameroun set houses ablaze in ndu and r...       1   \n",
       "20     cameroons bir soldiers on the 05012020 invaded...       1   \n",
       "21     as fires ablaze throughout the landas the prop...       1   \n",
       "22     thankfultuesday isaiah 432 when you pass throu...       0   \n",
       "23     when you walk through the fire you will not be...       0   \n",
       "24     originally they were intended to be fired at b...       1   \n",
       "25     warm greetings to all on the occasion of lohri...       0   \n",
       "26     another arson in njikomboyonwr the ambazombies...       1   \n",
       "27     another public market in haiti mysteriously se...       1   \n",
       "28                               that is kind true sadly       0   \n",
       "29            i swear that jam will set the world ablaze       0   \n",
       "30     marivan kurdistan province monday jan 13th 202...       1   \n",
       "...                                                  ...     ...   \n",
       "11336   admit it we are all bias wrecked by soobin today       0   \n",
       "11337  hi so ive started to stan the girl group ans b...       0   \n",
       "11338  kesian ular we have wrecked their natural habitat       1   \n",
       "11339  my twat of an art teacher  what grades do you ...       0   \n",
       "11340  my first listen was also in the whip i damn ne...       0   \n",
       "11341  hes not a journalist and doesnt pretend to be ...       0   \n",
       "11342  by all means give up sugar and carbs your body...       0   \n",
       "11343  chanyeol 2k19 gaming destroys me messed my min...       0   \n",
       "11344                    hell be wrecked if that happens       0   \n",
       "11345  hes the oxygen that pumps blood to my living h...       0   \n",
       "11346  when youre watching clemson get wrecked and se...       0   \n",
       "11347  why would operators put new buses on school co...       0   \n",
       "11348  democratic propaganda wrecked libya obama just...       0   \n",
       "11349   cc this is what i was telling you the other time       0   \n",
       "11350  tryna think of a plot but nothing comes to min...       0   \n",
       "11351               this was me when my car got wrecked        0   \n",
       "11352  since everyone is talking about chen and exo o...       0   \n",
       "11353  is it possible to be bias wrecked by your own ...       0   \n",
       "11354  yeah proper liverpool fans wrecked man citys b...       1   \n",
       "11355  trump and sisi rejected foreign exploitation a...       1   \n",
       "11358  i get wrecked too shake my head  my biases in ...       0   \n",
       "11359  trump and sisi rejected foreign exploitation a...       1   \n",
       "11360  man gogo version of gucci bandana just came on...       0   \n",
       "11362  stell wrecked ako palagi sayo haha alabtopspot...       0   \n",
       "11363  hes the oxygen that pumps blood to my living h...       0   \n",
       "11364  had these guys last game n fcked them talked n...       0   \n",
       "11365  media should have warned us well in advance th...       0   \n",
       "11366  i feel directly attacked  i consider moonbin  ...       0   \n",
       "11368  ok who remember outcast nd the dora au those a...       0   \n",
       "11369      jake corway wrecked while running 14th at irp       1   \n",
       "\n",
       "                                               tokenized  \\\n",
       "id                                                         \n",
       "0      [communal, violence, in, bhainsa, telangana, s...   \n",
       "1      [telangana, section, 144, has, been, imposed, ...   \n",
       "2         [arsonist, sets, cars, ablaze, at, dealership]   \n",
       "4      [lord, jesus, your, love, brings, freedom, and...   \n",
       "5      [if, this, child, was, chinese, this, tweet, w...   \n",
       "6      [several, houses, have, been, set, ablaze, in,...   \n",
       "7      [asansol, a, bjp, office, in, salanpur, villag...   \n",
       "8      [national, security, minister, kan, dapaahs, s...   \n",
       "9      [this, creature, whos, soul, is, no, longer, c...   \n",
       "10     [images, showing, the, havoc, caused, by, the,...   \n",
       "11     [social, media, went, bananas, after, chuba, h...   \n",
       "12     [hausa, youths, set, area, office, of, apapaig...   \n",
       "13     [under, mamatabanerjee, political, violence, v...   \n",
       "14          [amen, set, the, whole, system, ablaze, man]   \n",
       "15     [images, showing, the, havoc, caused, by, the,...   \n",
       "16     [no, cows, today, but, our, local, factory, is...   \n",
       "17     [rengoku, sets, my, heart, ablaze, postscript,...   \n",
       "18     [paulzizkaphoto, rundle, ablaze, wishing, you,...   \n",
       "19     [french, cameroun, set, houses, ablaze, in, nd...   \n",
       "20     [cameroons, bir, soldiers, on, the, 05012020, ...   \n",
       "21     [as, fires, ablaze, throughout, the, landas, t...   \n",
       "22     [thankfultuesday, isaiah, 432, when, you, pass...   \n",
       "23     [when, you, walk, through, the, fire, you, wil...   \n",
       "24     [originally, they, were, intended, to, be, fir...   \n",
       "25     [warm, greetings, to, all, on, the, occasion, ...   \n",
       "26     [another, arson, in, njikomboyonwr, the, ambaz...   \n",
       "27     [another, public, market, in, haiti, mysteriou...   \n",
       "28                         [that, is, kind, true, sadly]   \n",
       "29     [i, swear, that, jam, will, set, the, world, a...   \n",
       "30     [marivan, kurdistan, province, monday, jan, 13...   \n",
       "...                                                  ...   \n",
       "11336  [admit, it, we, are, all, bias, wrecked, by, s...   \n",
       "11337  [hi, so, ive, started, to, stan, the, girl, gr...   \n",
       "11338  [kesian, ular, we, have, wrecked, their, natur...   \n",
       "11339  [my, twat, of, an, art, teacher, what, grades,...   \n",
       "11340  [my, first, listen, was, also, in, the, whip, ...   \n",
       "11341  [hes, not, a, journalist, and, doesnt, pretend...   \n",
       "11342  [by, all, means, give, up, sugar, and, carbs, ...   \n",
       "11343  [chanyeol, 2k19, gaming, destroys, me, messed,...   \n",
       "11344             [hell, be, wrecked, if, that, happens]   \n",
       "11345  [hes, the, oxygen, that, pumps, blood, to, my,...   \n",
       "11346  [when, youre, watching, clemson, get, wrecked,...   \n",
       "11347  [why, would, operators, put, new, buses, on, s...   \n",
       "11348  [democratic, propaganda, wrecked, libya, obama...   \n",
       "11349  [cc, this, is, what, i, was, telling, you, the...   \n",
       "11350  [tryna, think, of, a, plot, but, nothing, come...   \n",
       "11351       [this, was, me, when, my, car, got, wrecked]   \n",
       "11352  [since, everyone, is, talking, about, chen, an...   \n",
       "11353  [is, it, possible, to, be, bias, wrecked, by, ...   \n",
       "11354  [yeah, proper, liverpool, fans, wrecked, man, ...   \n",
       "11355  [trump, and, sisi, rejected, foreign, exploita...   \n",
       "11358  [i, get, wrecked, too, shake, my, head, my, bi...   \n",
       "11359  [trump, and, sisi, rejected, foreign, exploita...   \n",
       "11360  [man, gogo, version, of, gucci, bandana, just,...   \n",
       "11362  [stell, wrecked, ako, palagi, sayo, haha, alab...   \n",
       "11363  [hes, the, oxygen, that, pumps, blood, to, my,...   \n",
       "11364  [had, these, guys, last, game, n, fcked, them,...   \n",
       "11365  [media, should, have, warned, us, well, in, ad...   \n",
       "11366  [i, feel, directly, attacked, i, consider, moo...   \n",
       "11368  [ok, who, remember, outcast, nd, the, dora, au...   \n",
       "11369  [jake, corway, wrecked, while, running, 14th, ...   \n",
       "\n",
       "                                           PorterStemmer  \n",
       "id                                                        \n",
       "0      [commun, violenc, in, bhainsa, telangana, ston...  \n",
       "1      [telangana, section, 144, ha, been, impos, in,...  \n",
       "2            [arsonist, set, car, ablaz, at, dealership]  \n",
       "4      [lord, jesu, your, love, bring, freedom, and, ...  \n",
       "5      [if, thi, child, wa, chines, thi, tweet, would...  \n",
       "6      [sever, hous, have, been, set, ablaz, in, ngem...  \n",
       "7      [asansol, a, bjp, offic, in, salanpur, villag,...  \n",
       "8      [nation, secur, minist, kan, dapaah, side, chi...  \n",
       "9      [thi, creatur, who, soul, is, no, longer, clar...  \n",
       "10     [imag, show, the, havoc, caus, by, the, camero...  \n",
       "11     [social, media, went, banana, after, chuba, hu...  \n",
       "12     [hausa, youth, set, area, offic, of, apapaigan...  \n",
       "13     [under, mamatabanerje, polit, violenc, vandal,...  \n",
       "14           [amen, set, the, whole, system, ablaz, man]  \n",
       "15     [imag, show, the, havoc, caus, by, the, camero...  \n",
       "16     [no, cow, today, but, our, local, factori, is,...  \n",
       "17     [rengoku, set, my, heart, ablaz, postscript, i...  \n",
       "18     [paulzizkaphoto, rundl, ablaz, wish, you, all,...  \n",
       "19     [french, cameroun, set, hous, ablaz, in, ndu, ...  \n",
       "20     [cameroon, bir, soldier, on, the, 05012020, in...  \n",
       "21     [as, fire, ablaz, throughout, the, landa, the,...  \n",
       "22     [thankfultuesday, isaiah, 432, when, you, pass...  \n",
       "23     [when, you, walk, through, the, fire, you, wil...  \n",
       "24     [origin, they, were, intend, to, be, fire, at,...  \n",
       "25     [warm, greet, to, all, on, the, occas, of, loh...  \n",
       "26     [anoth, arson, in, njikomboyonwr, the, ambazom...  \n",
       "27     [anoth, public, market, in, haiti, mysteri, se...  \n",
       "28                         [that, is, kind, true, sadli]  \n",
       "29     [i, swear, that, jam, will, set, the, world, a...  \n",
       "30     [marivan, kurdistan, provinc, monday, jan, 13t...  \n",
       "...                                                  ...  \n",
       "11336  [admit, it, we, are, all, bia, wreck, by, soob...  \n",
       "11337  [hi, so, ive, start, to, stan, the, girl, grou...  \n",
       "11338  [kesian, ular, we, have, wreck, their, natur, ...  \n",
       "11339  [my, twat, of, an, art, teacher, what, grade, ...  \n",
       "11340  [my, first, listen, wa, also, in, the, whip, i...  \n",
       "11341  [he, not, a, journalist, and, doesnt, pretend,...  \n",
       "11342  [by, all, mean, give, up, sugar, and, carb, yo...  \n",
       "11343  [chanyeol, 2k19, game, destroy, me, mess, my, ...  \n",
       "11344                [hell, be, wreck, if, that, happen]  \n",
       "11345  [he, the, oxygen, that, pump, blood, to, my, l...  \n",
       "11346  [when, your, watch, clemson, get, wreck, and, ...  \n",
       "11347  [whi, would, oper, put, new, buse, on, school,...  \n",
       "11348  [democrat, propaganda, wreck, libya, obama, ju...  \n",
       "11349  [cc, thi, is, what, i, wa, tell, you, the, oth...  \n",
       "11350  [tryna, think, of, a, plot, but, noth, come, t...  \n",
       "11351           [thi, wa, me, when, my, car, got, wreck]  \n",
       "11352  [sinc, everyon, is, talk, about, chen, and, ex...  \n",
       "11353  [is, it, possibl, to, be, bia, wreck, by, your...  \n",
       "11354  [yeah, proper, liverpool, fan, wreck, man, cit...  \n",
       "11355  [trump, and, sisi, reject, foreign, exploit, a...  \n",
       "11358  [i, get, wreck, too, shake, my, head, my, bias...  \n",
       "11359  [trump, and, sisi, reject, foreign, exploit, a...  \n",
       "11360  [man, gogo, version, of, gucci, bandana, just,...  \n",
       "11362  [stell, wreck, ako, palagi, sayo, haha, alabto...  \n",
       "11363  [he, the, oxygen, that, pump, blood, to, my, l...  \n",
       "11364  [had, these, guy, last, game, n, fcked, them, ...  \n",
       "11365  [media, should, have, warn, us, well, in, adva...  \n",
       "11366  [i, feel, directli, attack, i, consid, moonbin...  \n",
       "11368  [ok, who, rememb, outcast, nd, the, dora, au, ...  \n",
       "11369   [jake, corway, wreck, while, run, 14th, at, irp]  \n",
       "\n",
       "[10898 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PorterStemmer']=df['tokenized'].apply(porter_stemmer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speach (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def defualt_pos_tagger (text):\n",
    "    \n",
    "    #tags=[nltk.pos_tag(word) for word in text]\n",
    "    tags=nltk.pos_tag(text)\n",
    "    return tags\n",
    "\n",
    "df['defualt_postag']=df['tokenized'].apply(defualt_pos_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "#def pos_tag_wordnet(text, pos_tag_type='pos_tag'):\n",
    "def pos_tag_wordnet(text):\n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    # map the pos tagging output with wordnet output \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "    return pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_postag_wnet']=df['tokenized'].apply(pos_tag_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemma=[lemmatizer.lemmatize(word,tag) for word,tag in text]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization can be done with or without considering pos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization without considering pos:\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "df['lemmatized_word_without_pos']=df['tokenized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "#df['lemmatized_word_without_pos']=df['lemmatized_word_without_pos'].apply(lambda x: [word for word in x if word not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization with considering pos:\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "df['lemmatized_word_with_pos']=df['combined_postag_wnet'].apply(lambda x: lemmatize(x))\n",
    "\n",
    "# df['lemmatized_word_with_pos']=df['lemmatized_word_with_pos'].apply(lambda x: [word for word in x if word not in stop])\n",
    "df['lemmatized_text']=[' '.join(map(str,i)) for i in df['lemmatized_word_with_pos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>defualt_postag</th>\n",
       "      <th>combined_postag_wnet</th>\n",
       "      <th>lemmatized_word_without_pos</th>\n",
       "      <th>lemmatized_word_with_pos</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>communal violence in bhainsa telangana stones ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "      <td>[commun, violenc, in, bhainsa, telangana, ston...</td>\n",
       "      <td>[(communal, JJ), (violence, NN), (in, IN), (bh...</td>\n",
       "      <td>[(communal, n), (violence, n), (in, n), (bhain...</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "      <td>[communal, violence, in, bhainsa, telangana, s...</td>\n",
       "      <td>communal violence in bhainsa telangana stone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>telangana section 144 has been imposed in bhai...</td>\n",
       "      <td>1</td>\n",
       "      <td>[telangana, section, 144, has, been, imposed, ...</td>\n",
       "      <td>[telangana, section, 144, ha, been, impos, in,...</td>\n",
       "      <td>[(telangana, JJ), (section, NN), (144, CD), (h...</td>\n",
       "      <td>[(telangana, n), (section, n), (144, n), (has,...</td>\n",
       "      <td>[telangana, section, 144, ha, been, imposed, i...</td>\n",
       "      <td>[telangana, section, 144, ha, been, impose, in...</td>\n",
       "      <td>telangana section 144 ha been impose in bhains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>arsonist sets cars ablaze at dealership</td>\n",
       "      <td>1</td>\n",
       "      <td>[arsonist, sets, cars, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, set, car, ablaz, at, dealership]</td>\n",
       "      <td>[(arsonist, JJ), (sets, NNS), (cars, NNS), (ab...</td>\n",
       "      <td>[(arsonist, n), (sets, v), (cars, n), (ablaze,...</td>\n",
       "      <td>[arsonist, set, car, ablaze, at, dealership]</td>\n",
       "      <td>[arsonist, set, car, ablaze, at, dealership]</td>\n",
       "      <td>arsonist set car ablaze at dealership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lord jesus your love brings freedom and pardon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "      <td>[lord, jesu, your, love, bring, freedom, and, ...</td>\n",
       "      <td>[(lord, NN), (jesus, VBZ), (your, PRP$), (love...</td>\n",
       "      <td>[(lord, n), (jesus, n), (your, n), (love, v), ...</td>\n",
       "      <td>[lord, jesus, your, love, brings, freedom, and...</td>\n",
       "      <td>[lord, jesus, your, love, bring, freedom, and,...</td>\n",
       "      <td>lord jesus your love bring freedom and pardon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>OC</td>\n",
       "      <td>if this child was chinese this tweet would hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, this, child, was, chinese, this, tweet, w...</td>\n",
       "      <td>[if, thi, child, wa, chines, thi, tweet, would...</td>\n",
       "      <td>[(if, IN), (this, DT), (child, NN), (was, VBD)...</td>\n",
       "      <td>[(if, n), (this, n), (child, n), (was, n), (ch...</td>\n",
       "      <td>[if, this, child, wa, chinese, this, tweet, wo...</td>\n",
       "      <td>[if, this, child, wa, chinese, this, tweet, wo...</td>\n",
       "      <td>if this child wa chinese this tweet would have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, England</td>\n",
       "      <td>several houses have been set ablaze in ngemsib...</td>\n",
       "      <td>1</td>\n",
       "      <td>[several, houses, have, been, set, ablaze, in,...</td>\n",
       "      <td>[sever, hous, have, been, set, ablaz, in, ngem...</td>\n",
       "      <td>[(several, JJ), (houses, NNS), (have, VBP), (b...</td>\n",
       "      <td>[(several, n), (houses, n), (have, n), (been, ...</td>\n",
       "      <td>[several, house, have, been, set, ablaze, in, ...</td>\n",
       "      <td>[several, house, have, been, set, ablaze, in, ...</td>\n",
       "      <td>several house have been set ablaze in ngemsiba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Bharat</td>\n",
       "      <td>asansol a bjp office in salanpur village was s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[asansol, a, bjp, office, in, salanpur, villag...</td>\n",
       "      <td>[asansol, a, bjp, offic, in, salanpur, villag,...</td>\n",
       "      <td>[(asansol, VB), (a, DT), (bjp, NN), (office, N...</td>\n",
       "      <td>[(asansol, n), (a, n), (bjp, n), (office, n), ...</td>\n",
       "      <td>[asansol, a, bjp, office, in, salanpur, villag...</td>\n",
       "      <td>[asansol, a, bjp, office, in, salanpur, villag...</td>\n",
       "      <td>asansol a bjp office in salanpur village wa se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Accra, Ghana</td>\n",
       "      <td>national security minister kan dapaahs side ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>[national, security, minister, kan, dapaahs, s...</td>\n",
       "      <td>[nation, secur, minist, kan, dapaah, side, chi...</td>\n",
       "      <td>[(national, JJ), (security, NN), (minister, NN...</td>\n",
       "      <td>[(national, a), (security, n), (minister, n), ...</td>\n",
       "      <td>[national, security, minister, kan, dapaahs, s...</td>\n",
       "      <td>[national, security, minister, kan, dapaahs, s...</td>\n",
       "      <td>national security minister kan dapaahs side ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Searching</td>\n",
       "      <td>this creature whos soul is no longer clarent b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, creature, whos, soul, is, no, longer, c...</td>\n",
       "      <td>[thi, creatur, who, soul, is, no, longer, clar...</td>\n",
       "      <td>[(this, DT), (creature, NN), (whos, JJ), (soul...</td>\n",
       "      <td>[(this, n), (creature, n), (whos, n), (soul, n...</td>\n",
       "      <td>[this, creature, who, soul, is, no, longer, cl...</td>\n",
       "      <td>[this, creature, who, soul, is, no, longer, cl...</td>\n",
       "      <td>this creature who soul is no longer clarent bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images showing the havoc caused by the cameroo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[images, showing, the, havoc, caused, by, the,...</td>\n",
       "      <td>[imag, show, the, havoc, caus, by, the, camero...</td>\n",
       "      <td>[(images, NNS), (showing, VBG), (the, DT), (ha...</td>\n",
       "      <td>[(images, n), (showing, v), (the, n), (havoc, ...</td>\n",
       "      <td>[image, showing, the, havoc, caused, by, the, ...</td>\n",
       "      <td>[image, show, the, havoc, cause, by, the, came...</td>\n",
       "      <td>image show the havoc cause by the cameroon mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>social media went bananas after chuba hubbard ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[social, media, went, bananas, after, chuba, h...</td>\n",
       "      <td>[social, media, went, banana, after, chuba, hu...</td>\n",
       "      <td>[(social, JJ), (media, NNS), (went, VBD), (ban...</td>\n",
       "      <td>[(social, a), (media, n), (went, v), (bananas,...</td>\n",
       "      <td>[social, medium, went, banana, after, chuba, h...</td>\n",
       "      <td>[social, medium, go, banana, after, chuba, hub...</td>\n",
       "      <td>social medium go banana after chuba hubbard an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hausa youths set area office of apapaiganmu lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hausa, youths, set, area, office, of, apapaig...</td>\n",
       "      <td>[hausa, youth, set, area, offic, of, apapaigan...</td>\n",
       "      <td>[(hausa, JJ), (youths, NNS), (set, VBN), (area...</td>\n",
       "      <td>[(hausa, n), (youths, n), (set, v), (area, n),...</td>\n",
       "      <td>[hausa, youth, set, area, office, of, apapaiga...</td>\n",
       "      <td>[hausa, youth, set, area, office, of, apapaiga...</td>\n",
       "      <td>hausa youth set area office of apapaiganmu loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>HYDERABAD</td>\n",
       "      <td>under mamatabanerjee political violence  vanda...</td>\n",
       "      <td>1</td>\n",
       "      <td>[under, mamatabanerjee, political, violence, v...</td>\n",
       "      <td>[under, mamatabanerje, polit, violenc, vandal,...</td>\n",
       "      <td>[(under, IN), (mamatabanerjee, JJ), (political...</td>\n",
       "      <td>[(under, n), (mamatabanerjee, n), (political, ...</td>\n",
       "      <td>[under, mamatabanerjee, political, violence, v...</td>\n",
       "      <td>[under, mamatabanerjee, political, violence, v...</td>\n",
       "      <td>under mamatabanerjee political violence vandal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Reno, NV</td>\n",
       "      <td>amen set the whole system ablaze man</td>\n",
       "      <td>0</td>\n",
       "      <td>[amen, set, the, whole, system, ablaze, man]</td>\n",
       "      <td>[amen, set, the, whole, system, ablaz, man]</td>\n",
       "      <td>[(amen, NNS), (set, VBD), (the, DT), (whole, J...</td>\n",
       "      <td>[(amen, n), (set, v), (the, n), (whole, a), (s...</td>\n",
       "      <td>[amen, set, the, whole, system, ablaze, man]</td>\n",
       "      <td>[amen, set, the, whole, system, ablaze, man]</td>\n",
       "      <td>amen set the whole system ablaze man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images showing the havoc caused by the cameroo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[images, showing, the, havoc, caused, by, the,...</td>\n",
       "      <td>[imag, show, the, havoc, caus, by, the, camero...</td>\n",
       "      <td>[(images, NNS), (showing, VBG), (the, DT), (ha...</td>\n",
       "      <td>[(images, n), (showing, v), (the, n), (havoc, ...</td>\n",
       "      <td>[image, showing, the, havoc, caused, by, the, ...</td>\n",
       "      <td>[image, show, the, havoc, cause, by, the, came...</td>\n",
       "      <td>image show the havoc cause by the cameroon mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no cows today but our local factory is sadly s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[no, cows, today, but, our, local, factory, is...</td>\n",
       "      <td>[no, cow, today, but, our, local, factori, is,...</td>\n",
       "      <td>[(no, DT), (cows, NNS), (today, NN), (but, CC)...</td>\n",
       "      <td>[(no, n), (cows, n), (today, n), (but, n), (ou...</td>\n",
       "      <td>[no, cow, today, but, our, local, factory, is,...</td>\n",
       "      <td>[no, cow, today, but, our, local, factory, is,...</td>\n",
       "      <td>no cow today but our local factory is sadly st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rengoku sets my heart ablaze postscript i miss...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rengoku, sets, my, heart, ablaze, postscript,...</td>\n",
       "      <td>[rengoku, set, my, heart, ablaz, postscript, i...</td>\n",
       "      <td>[(rengoku, NN), (sets, NNS), (my, PRP$), (hear...</td>\n",
       "      <td>[(rengoku, n), (sets, v), (my, n), (heart, n),...</td>\n",
       "      <td>[rengoku, set, my, heart, ablaze, postscript, ...</td>\n",
       "      <td>[rengoku, set, my, heart, ablaze, postscript, ...</td>\n",
       "      <td>rengoku set my heart ablaze postscript i miss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>paulzizkaphoto rundle ablaze wishing you all a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[paulzizkaphoto, rundle, ablaze, wishing, you,...</td>\n",
       "      <td>[paulzizkaphoto, rundl, ablaz, wish, you, all,...</td>\n",
       "      <td>[(paulzizkaphoto, NN), (rundle, NN), (ablaze, ...</td>\n",
       "      <td>[(paulzizkaphoto, n), (rundle, n), (ablaze, r)...</td>\n",
       "      <td>[paulzizkaphoto, rundle, ablaze, wishing, you,...</td>\n",
       "      <td>[paulzizkaphoto, rundle, ablaze, wishing, you,...</td>\n",
       "      <td>paulzizkaphoto rundle ablaze wishing you all a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>french cameroun set houses ablaze in ndu and r...</td>\n",
       "      <td>1</td>\n",
       "      <td>[french, cameroun, set, houses, ablaze, in, nd...</td>\n",
       "      <td>[french, cameroun, set, hous, ablaz, in, ndu, ...</td>\n",
       "      <td>[(french, JJ), (cameroun, NN), (set, VBN), (ho...</td>\n",
       "      <td>[(french, n), (cameroun, n), (set, v), (houses...</td>\n",
       "      <td>[french, cameroun, set, house, ablaze, in, ndu...</td>\n",
       "      <td>[french, cameroun, set, house, ablaze, in, ndu...</td>\n",
       "      <td>french cameroun set house ablaze in ndu and ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cameroons bir soldiers on the 05012020 invaded...</td>\n",
       "      <td>1</td>\n",
       "      <td>[cameroons, bir, soldiers, on, the, 05012020, ...</td>\n",
       "      <td>[cameroon, bir, soldier, on, the, 05012020, in...</td>\n",
       "      <td>[(cameroons, NNS), (bir, VBP), (soldiers, NNS)...</td>\n",
       "      <td>[(cameroons, n), (bir, n), (soldiers, n), (on,...</td>\n",
       "      <td>[cameroon, bir, soldier, on, the, 05012020, in...</td>\n",
       "      <td>[cameroon, bir, soldier, on, the, 05012020, in...</td>\n",
       "      <td>cameroon bir soldier on the 05012020 invade th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>as fires ablaze throughout the landas the prop...</td>\n",
       "      <td>1</td>\n",
       "      <td>[as, fires, ablaze, throughout, the, landas, t...</td>\n",
       "      <td>[as, fire, ablaz, throughout, the, landa, the,...</td>\n",
       "      <td>[(as, IN), (fires, NNS), (ablaze, VBP), (throu...</td>\n",
       "      <td>[(as, n), (fires, n), (ablaze, r), (throughout...</td>\n",
       "      <td>[a, fire, ablaze, throughout, the, landas, the...</td>\n",
       "      <td>[a, fire, ablaze, throughout, the, landas, the...</td>\n",
       "      <td>a fire ablaze throughout the landas the propho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Italy</td>\n",
       "      <td>thankfultuesday isaiah 432 when you pass throu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[thankfultuesday, isaiah, 432, when, you, pass...</td>\n",
       "      <td>[thankfultuesday, isaiah, 432, when, you, pass...</td>\n",
       "      <td>[(thankfultuesday, NN), (isaiah, VBZ), (432, C...</td>\n",
       "      <td>[(thankfultuesday, n), (isaiah, n), (432, n), ...</td>\n",
       "      <td>[thankfultuesday, isaiah, 432, when, you, pas,...</td>\n",
       "      <td>[thankfultuesday, isaiah, 432, when, you, pass...</td>\n",
       "      <td>thankfultuesday isaiah 432 when you pass throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>` ˗ˏˋ i'm⠀waiting⠀for⠀you⠀to⠀pour⠀my⠀𝘀𝗶𝗻𝘀⠀onto...</td>\n",
       "      <td>when you walk through the fire you will not be...</td>\n",
       "      <td>0</td>\n",
       "      <td>[when, you, walk, through, the, fire, you, wil...</td>\n",
       "      <td>[when, you, walk, through, the, fire, you, wil...</td>\n",
       "      <td>[(when, WRB), (you, PRP), (walk, VBP), (throug...</td>\n",
       "      <td>[(when, n), (you, n), (walk, v), (through, n),...</td>\n",
       "      <td>[when, you, walk, through, the, fire, you, wil...</td>\n",
       "      <td>[when, you, walk, through, the, fire, you, wil...</td>\n",
       "      <td>when you walk through the fire you will not be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Okielahoma</td>\n",
       "      <td>originally they were intended to be fired at b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[originally, they, were, intended, to, be, fir...</td>\n",
       "      <td>[origin, they, were, intend, to, be, fire, at,...</td>\n",
       "      <td>[(originally, RB), (they, PRP), (were, VBD), (...</td>\n",
       "      <td>[(originally, r), (they, n), (were, n), (inten...</td>\n",
       "      <td>[originally, they, were, intended, to, be, fir...</td>\n",
       "      <td>[originally, they, were, intend, to, be, fire,...</td>\n",
       "      <td>originally they were intend to be fire at boar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>warm greetings to all on the occasion of lohri...</td>\n",
       "      <td>0</td>\n",
       "      <td>[warm, greetings, to, all, on, the, occasion, ...</td>\n",
       "      <td>[warm, greet, to, all, on, the, occas, of, loh...</td>\n",
       "      <td>[(warm, JJ), (greetings, NNS), (to, TO), (all,...</td>\n",
       "      <td>[(warm, a), (greetings, n), (to, n), (all, n),...</td>\n",
       "      <td>[warm, greeting, to, all, on, the, occasion, o...</td>\n",
       "      <td>[warm, greeting, to, all, on, the, occasion, o...</td>\n",
       "      <td>warm greeting to all on the occasion of lohri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>another arson in njikomboyonwr the ambazombies...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, arson, in, njikomboyonwr, the, ambaz...</td>\n",
       "      <td>[anoth, arson, in, njikomboyonwr, the, ambazom...</td>\n",
       "      <td>[(another, DT), (arson, NN), (in, IN), (njikom...</td>\n",
       "      <td>[(another, n), (arson, n), (in, n), (njikomboy...</td>\n",
       "      <td>[another, arson, in, njikomboyonwr, the, ambaz...</td>\n",
       "      <td>[another, arson, in, njikomboyonwr, the, ambaz...</td>\n",
       "      <td>another arson in njikomboyonwr the ambazombies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Havana 3am</td>\n",
       "      <td>another public market in haiti mysteriously se...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, public, market, in, haiti, mysteriou...</td>\n",
       "      <td>[anoth, public, market, in, haiti, mysteri, se...</td>\n",
       "      <td>[(another, DT), (public, JJ), (market, NN), (i...</td>\n",
       "      <td>[(another, n), (public, a), (market, n), (in, ...</td>\n",
       "      <td>[another, public, market, in, haiti, mysteriou...</td>\n",
       "      <td>[another, public, market, in, haiti, mysteriou...</td>\n",
       "      <td>another public market in haiti mysteriously se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>that is kind true sadly</td>\n",
       "      <td>0</td>\n",
       "      <td>[that, is, kind, true, sadly]</td>\n",
       "      <td>[that, is, kind, true, sadli]</td>\n",
       "      <td>[(that, DT), (is, VBZ), (kind, NN), (true, JJ)...</td>\n",
       "      <td>[(that, n), (is, n), (kind, n), (true, a), (sa...</td>\n",
       "      <td>[that, is, kind, true, sadly]</td>\n",
       "      <td>[that, is, kind, true, sadly]</td>\n",
       "      <td>that is kind true sadly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i swear that jam will set the world ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, swear, that, jam, will, set, the, world, a...</td>\n",
       "      <td>[i, swear, that, jam, will, set, the, world, a...</td>\n",
       "      <td>[(i, NN), (swear, VBP), (that, IN), (jam, NN),...</td>\n",
       "      <td>[(i, n), (swear, n), (that, n), (jam, n), (wil...</td>\n",
       "      <td>[i, swear, that, jam, will, set, the, world, a...</td>\n",
       "      <td>[i, swear, that, jam, will, set, the, world, a...</td>\n",
       "      <td>i swear that jam will set the world ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marivan kurdistan province monday jan 13th 202...</td>\n",
       "      <td>1</td>\n",
       "      <td>[marivan, kurdistan, province, monday, jan, 13...</td>\n",
       "      <td>[marivan, kurdistan, provinc, monday, jan, 13t...</td>\n",
       "      <td>[(marivan, NN), (kurdistan, NN), (province, NN...</td>\n",
       "      <td>[(marivan, n), (kurdistan, n), (province, n), ...</td>\n",
       "      <td>[marivan, kurdistan, province, monday, jan, 13...</td>\n",
       "      <td>[marivan, kurdistan, province, monday, jan, 13...</td>\n",
       "      <td>marivan kurdistan province monday jan 13th 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11336</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>𝕸𝖔𝖆𝖗𝖒𝖞</td>\n",
       "      <td>admit it we are all bias wrecked by soobin today</td>\n",
       "      <td>0</td>\n",
       "      <td>[admit, it, we, are, all, bias, wrecked, by, s...</td>\n",
       "      <td>[admit, it, we, are, all, bia, wreck, by, soob...</td>\n",
       "      <td>[(admit, NN), (it, PRP), (we, PRP), (are, VBP)...</td>\n",
       "      <td>[(admit, v), (it, n), (we, n), (are, n), (all,...</td>\n",
       "      <td>[admit, it, we, are, all, bias, wrecked, by, s...</td>\n",
       "      <td>[admit, it, we, are, all, bias, wreck, by, soo...</td>\n",
       "      <td>admit it we are all bias wreck by soobin today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11337</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Malilipot, Bicol Region</td>\n",
       "      <td>hi so ive started to stan the girl group ans b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hi, so, ive, started, to, stan, the, girl, gr...</td>\n",
       "      <td>[hi, so, ive, start, to, stan, the, girl, grou...</td>\n",
       "      <td>[(hi, NNS), (so, RB), (ive, JJ), (started, VBD...</td>\n",
       "      <td>[(hi, n), (so, n), (ive, n), (started, v), (to...</td>\n",
       "      <td>[hi, so, ive, started, to, stan, the, girl, gr...</td>\n",
       "      <td>[hi, so, ive, start, to, stan, the, girl, grou...</td>\n",
       "      <td>hi so ive start to stan the girl group an beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11338</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Puchong</td>\n",
       "      <td>kesian ular we have wrecked their natural habitat</td>\n",
       "      <td>1</td>\n",
       "      <td>[kesian, ular, we, have, wrecked, their, natur...</td>\n",
       "      <td>[kesian, ular, we, have, wreck, their, natur, ...</td>\n",
       "      <td>[(kesian, JJ), (ular, NN), (we, PRP), (have, V...</td>\n",
       "      <td>[(kesian, n), (ular, n), (we, n), (have, n), (...</td>\n",
       "      <td>[kesian, ular, we, have, wrecked, their, natur...</td>\n",
       "      <td>[kesian, ular, we, have, wreck, their, natural...</td>\n",
       "      <td>kesian ular we have wreck their natural habitat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11339</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>fan acc</td>\n",
       "      <td>my twat of an art teacher  what grades do you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, twat, of, an, art, teacher, what, grades,...</td>\n",
       "      <td>[my, twat, of, an, art, teacher, what, grade, ...</td>\n",
       "      <td>[(my, PRP$), (twat, NN), (of, IN), (an, DT), (...</td>\n",
       "      <td>[(my, n), (twat, n), (of, n), (an, n), (art, n...</td>\n",
       "      <td>[my, twat, of, an, art, teacher, what, grade, ...</td>\n",
       "      <td>[my, twat, of, an, art, teacher, what, grade, ...</td>\n",
       "      <td>my twat of an art teacher what grade do you ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11340</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "      <td>my first listen was also in the whip i damn ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>[my, first, listen, was, also, in, the, whip, ...</td>\n",
       "      <td>[my, first, listen, wa, also, in, the, whip, i...</td>\n",
       "      <td>[(my, PRP$), (first, JJ), (listen, NN), (was, ...</td>\n",
       "      <td>[(my, n), (first, n), (listen, v), (was, n), (...</td>\n",
       "      <td>[my, first, listen, wa, also, in, the, whip, i...</td>\n",
       "      <td>[my, first, listen, wa, also, in, the, whip, i...</td>\n",
       "      <td>my first listen wa also in the whip i damn nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11341</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hes not a journalist and doesnt pretend to be ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hes, not, a, journalist, and, doesnt, pretend...</td>\n",
       "      <td>[he, not, a, journalist, and, doesnt, pretend,...</td>\n",
       "      <td>[(hes, NNS), (not, RB), (a, DT), (journalist, ...</td>\n",
       "      <td>[(hes, n), (not, n), (a, n), (journalist, n), ...</td>\n",
       "      <td>[he, not, a, journalist, and, doesnt, pretend,...</td>\n",
       "      <td>[he, not, a, journalist, and, doesnt, pretend,...</td>\n",
       "      <td>he not a journalist and doesnt pretend to be h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11342</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Shropshire</td>\n",
       "      <td>by all means give up sugar and carbs your body...</td>\n",
       "      <td>0</td>\n",
       "      <td>[by, all, means, give, up, sugar, and, carbs, ...</td>\n",
       "      <td>[by, all, mean, give, up, sugar, and, carb, yo...</td>\n",
       "      <td>[(by, IN), (all, DT), (means, NNS), (give, VBP...</td>\n",
       "      <td>[(by, n), (all, n), (means, n), (give, v), (up...</td>\n",
       "      <td>[by, all, mean, give, up, sugar, and, carbs, y...</td>\n",
       "      <td>[by, all, mean, give, up, sugar, and, carbs, y...</td>\n",
       "      <td>by all mean give up sugar and carbs your body ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>EXO's 💜</td>\n",
       "      <td>chanyeol 2k19 gaming destroys me messed my min...</td>\n",
       "      <td>0</td>\n",
       "      <td>[chanyeol, 2k19, gaming, destroys, me, messed,...</td>\n",
       "      <td>[chanyeol, 2k19, game, destroy, me, mess, my, ...</td>\n",
       "      <td>[(chanyeol, NN), (2k19, CD), (gaming, NN), (de...</td>\n",
       "      <td>[(chanyeol, n), (2k19, n), (gaming, n), (destr...</td>\n",
       "      <td>[chanyeol, 2k19, gaming, destroys, me, messed,...</td>\n",
       "      <td>[chanyeol, 2k19, gaming, destroys, me, messed,...</td>\n",
       "      <td>chanyeol 2k19 gaming destroys me messed my min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11344</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>🇲🇾</td>\n",
       "      <td>hell be wrecked if that happens</td>\n",
       "      <td>0</td>\n",
       "      <td>[hell, be, wrecked, if, that, happens]</td>\n",
       "      <td>[hell, be, wreck, if, that, happen]</td>\n",
       "      <td>[(hell, NN), (be, VB), (wrecked, VBN), (if, IN...</td>\n",
       "      <td>[(hell, n), (be, n), (wrecked, v), (if, n), (t...</td>\n",
       "      <td>[hell, be, wrecked, if, that, happens]</td>\n",
       "      <td>[hell, be, wreck, if, that, happen]</td>\n",
       "      <td>hell be wreck if that happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11345</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hes the oxygen that pumps blood to my living h...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hes, the, oxygen, that, pumps, blood, to, my,...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "      <td>[(hes, NNS), (the, DT), (oxygen, NN), (that, W...</td>\n",
       "      <td>[(hes, n), (the, n), (oxygen, n), (that, n), (...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "      <td>he the oxygen that pump blood to my live heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11346</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Alabama, USA</td>\n",
       "      <td>when youre watching clemson get wrecked and se...</td>\n",
       "      <td>0</td>\n",
       "      <td>[when, youre, watching, clemson, get, wrecked,...</td>\n",
       "      <td>[when, your, watch, clemson, get, wreck, and, ...</td>\n",
       "      <td>[(when, WRB), (youre, NN), (watching, VBG), (c...</td>\n",
       "      <td>[(when, n), (youre, n), (watching, v), (clemso...</td>\n",
       "      <td>[when, youre, watching, clemson, get, wrecked,...</td>\n",
       "      <td>[when, youre, watch, clemson, get, wreck, and,...</td>\n",
       "      <td>when youre watch clemson get wreck and see the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11347</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>why would operators put new buses on school co...</td>\n",
       "      <td>0</td>\n",
       "      <td>[why, would, operators, put, new, buses, on, s...</td>\n",
       "      <td>[whi, would, oper, put, new, buse, on, school,...</td>\n",
       "      <td>[(why, WRB), (would, MD), (operators, NNS), (p...</td>\n",
       "      <td>[(why, n), (would, n), (operators, n), (put, v...</td>\n",
       "      <td>[why, would, operator, put, new, bus, on, scho...</td>\n",
       "      <td>[why, would, operator, put, new, bus, on, scho...</td>\n",
       "      <td>why would operator put new bus on school contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11348</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Capon Bridge, WV,</td>\n",
       "      <td>democratic propaganda wrecked libya obama just...</td>\n",
       "      <td>0</td>\n",
       "      <td>[democratic, propaganda, wrecked, libya, obama...</td>\n",
       "      <td>[democrat, propaganda, wreck, libya, obama, ju...</td>\n",
       "      <td>[(democratic, JJ), (propaganda, NN), (wrecked,...</td>\n",
       "      <td>[(democratic, a), (propaganda, n), (wrecked, v...</td>\n",
       "      <td>[democratic, propaganda, wrecked, libya, obama...</td>\n",
       "      <td>[democratic, propaganda, wreck, libya, obama, ...</td>\n",
       "      <td>democratic propaganda wreck libya obama just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11349</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Zion, Nigeria</td>\n",
       "      <td>cc this is what i was telling you the other time</td>\n",
       "      <td>0</td>\n",
       "      <td>[cc, this, is, what, i, was, telling, you, the...</td>\n",
       "      <td>[cc, thi, is, what, i, wa, tell, you, the, oth...</td>\n",
       "      <td>[(cc, NN), (this, DT), (is, VBZ), (what, WP), ...</td>\n",
       "      <td>[(cc, n), (this, n), (is, n), (what, n), (i, n...</td>\n",
       "      <td>[cc, this, is, what, i, wa, telling, you, the,...</td>\n",
       "      <td>[cc, this, is, what, i, wa, tell, you, the, ot...</td>\n",
       "      <td>cc this is what i wa tell you the other time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11350</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tryna think of a plot but nothing comes to min...</td>\n",
       "      <td>0</td>\n",
       "      <td>[tryna, think, of, a, plot, but, nothing, come...</td>\n",
       "      <td>[tryna, think, of, a, plot, but, noth, come, t...</td>\n",
       "      <td>[(tryna, JJ), (think, NN), (of, IN), (a, DT), ...</td>\n",
       "      <td>[(tryna, n), (think, v), (of, n), (a, n), (plo...</td>\n",
       "      <td>[tryna, think, of, a, plot, but, nothing, come...</td>\n",
       "      <td>[tryna, think, of, a, plot, but, nothing, come...</td>\n",
       "      <td>tryna think of a plot but nothing come to mind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11351</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this was me when my car got wrecked</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, was, me, when, my, car, got, wrecked]</td>\n",
       "      <td>[thi, wa, me, when, my, car, got, wreck]</td>\n",
       "      <td>[(this, DT), (was, VBD), (me, PRP), (when, WRB...</td>\n",
       "      <td>[(this, n), (was, n), (me, n), (when, n), (my,...</td>\n",
       "      <td>[this, wa, me, when, my, car, got, wrecked]</td>\n",
       "      <td>[this, wa, me, when, my, car, get, wreck]</td>\n",
       "      <td>this wa me when my car get wreck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11352</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>since everyone is talking about chen and exo o...</td>\n",
       "      <td>0</td>\n",
       "      <td>[since, everyone, is, talking, about, chen, an...</td>\n",
       "      <td>[sinc, everyon, is, talk, about, chen, and, ex...</td>\n",
       "      <td>[(since, IN), (everyone, NN), (is, VBZ), (talk...</td>\n",
       "      <td>[(since, n), (everyone, n), (is, n), (talking,...</td>\n",
       "      <td>[since, everyone, is, talking, about, chen, an...</td>\n",
       "      <td>[since, everyone, is, talk, about, chen, and, ...</td>\n",
       "      <td>since everyone is talk about chen and exo on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11353</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>magic shop</td>\n",
       "      <td>is it possible to be bias wrecked by your own ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, it, possible, to, be, bias, wrecked, by, ...</td>\n",
       "      <td>[is, it, possibl, to, be, bia, wreck, by, your...</td>\n",
       "      <td>[(is, VBZ), (it, PRP), (possible, JJ), (to, TO...</td>\n",
       "      <td>[(is, n), (it, n), (possible, a), (to, n), (be...</td>\n",
       "      <td>[is, it, possible, to, be, bias, wrecked, by, ...</td>\n",
       "      <td>[is, it, possible, to, be, bias, wreck, by, yo...</td>\n",
       "      <td>is it possible to be bias wreck by your own bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11354</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yeah proper liverpool fans wrecked man citys b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[yeah, proper, liverpool, fans, wrecked, man, ...</td>\n",
       "      <td>[yeah, proper, liverpool, fan, wreck, man, cit...</td>\n",
       "      <td>[(yeah, UH), (proper, JJ), (liverpool, NN), (f...</td>\n",
       "      <td>[(yeah, n), (proper, a), (liverpool, n), (fans...</td>\n",
       "      <td>[yeah, proper, liverpool, fan, wrecked, man, c...</td>\n",
       "      <td>[yeah, proper, liverpool, fan, wreck, man, cit...</td>\n",
       "      <td>yeah proper liverpool fan wreck man city bus a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11355</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Recife</td>\n",
       "      <td>trump and sisi rejected foreign exploitation a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[trump, and, sisi, rejected, foreign, exploita...</td>\n",
       "      <td>[trump, and, sisi, reject, foreign, exploit, a...</td>\n",
       "      <td>[(trump, NN), (and, CC), (sisi, NN), (rejected...</td>\n",
       "      <td>[(trump, n), (and, n), (sisi, n), (rejected, v...</td>\n",
       "      <td>[trump, and, sisi, rejected, foreign, exploita...</td>\n",
       "      <td>[trump, and, sisi, reject, foreign, exploitati...</td>\n",
       "      <td>trump and sisi reject foreign exploitation and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11358</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>gguk's pocket ♡</td>\n",
       "      <td>i get wrecked too shake my head  my biases in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, get, wrecked, too, shake, my, head, my, bi...</td>\n",
       "      <td>[i, get, wreck, too, shake, my, head, my, bias...</td>\n",
       "      <td>[(i, NN), (get, VBP), (wrecked, VBN), (too, RB...</td>\n",
       "      <td>[(i, n), (get, v), (wrecked, v), (too, n), (sh...</td>\n",
       "      <td>[i, get, wrecked, too, shake, my, head, my, bi...</td>\n",
       "      <td>[i, get, wreck, too, shake, my, head, my, bias...</td>\n",
       "      <td>i get wreck too shake my head my bias in skz a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11359</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>trump and sisi rejected foreign exploitation a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[trump, and, sisi, rejected, foreign, exploita...</td>\n",
       "      <td>[trump, and, sisi, reject, foreign, exploit, a...</td>\n",
       "      <td>[(trump, NN), (and, CC), (sisi, NN), (rejected...</td>\n",
       "      <td>[(trump, n), (and, n), (sisi, n), (rejected, v...</td>\n",
       "      <td>[trump, and, sisi, rejected, foreign, exploita...</td>\n",
       "      <td>[trump, and, sisi, reject, foreign, exploitati...</td>\n",
       "      <td>trump and sisi reject foreign exploitation and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11360</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>D(M)V</td>\n",
       "      <td>man gogo version of gucci bandana just came on...</td>\n",
       "      <td>0</td>\n",
       "      <td>[man, gogo, version, of, gucci, bandana, just,...</td>\n",
       "      <td>[man, gogo, version, of, gucci, bandana, just,...</td>\n",
       "      <td>[(man, NN), (gogo, VBZ), (version, NN), (of, I...</td>\n",
       "      <td>[(man, n), (gogo, n), (version, n), (of, n), (...</td>\n",
       "      <td>[man, gogo, version, of, gucci, bandana, just,...</td>\n",
       "      <td>[man, gogo, version, of, gucci, bandana, just,...</td>\n",
       "      <td>man gogo version of gucci bandana just come on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11362</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>feuille d'érable</td>\n",
       "      <td>stell wrecked ako palagi sayo haha alabtopspot...</td>\n",
       "      <td>0</td>\n",
       "      <td>[stell, wrecked, ako, palagi, sayo, haha, alab...</td>\n",
       "      <td>[stell, wreck, ako, palagi, sayo, haha, alabto...</td>\n",
       "      <td>[(stell, NN), (wrecked, VBD), (ako, JJ), (pala...</td>\n",
       "      <td>[(stell, n), (wrecked, v), (ako, n), (palagi, ...</td>\n",
       "      <td>[stell, wrecked, ako, palagi, sayo, haha, alab...</td>\n",
       "      <td>[stell, wreck, ako, palagi, sayo, haha, alabto...</td>\n",
       "      <td>stell wreck ako palagi sayo haha alabtopspoton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11363</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hes the oxygen that pumps blood to my living h...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hes, the, oxygen, that, pumps, blood, to, my,...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "      <td>[(hes, NNS), (the, DT), (oxygen, NN), (that, W...</td>\n",
       "      <td>[(hes, n), (the, n), (oxygen, n), (that, n), (...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "      <td>[he, the, oxygen, that, pump, blood, to, my, l...</td>\n",
       "      <td>he the oxygen that pump blood to my live heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11364</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>had these guys last game n fcked them talked n...</td>\n",
       "      <td>0</td>\n",
       "      <td>[had, these, guys, last, game, n, fcked, them,...</td>\n",
       "      <td>[had, these, guy, last, game, n, fcked, them, ...</td>\n",
       "      <td>[(had, VBD), (these, DT), (guys, NNS), (last, ...</td>\n",
       "      <td>[(had, n), (these, n), (guys, n), (last, n), (...</td>\n",
       "      <td>[had, these, guy, last, game, n, fcked, them, ...</td>\n",
       "      <td>[had, these, guy, last, game, n, fcked, them, ...</td>\n",
       "      <td>had these guy last game n fcked them talk nons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Blue State in a red sea</td>\n",
       "      <td>media should have warned us well in advance th...</td>\n",
       "      <td>0</td>\n",
       "      <td>[media, should, have, warned, us, well, in, ad...</td>\n",
       "      <td>[media, should, have, warn, us, well, in, adva...</td>\n",
       "      <td>[(media, NNS), (should, MD), (have, VB), (warn...</td>\n",
       "      <td>[(media, n), (should, n), (have, n), (warned, ...</td>\n",
       "      <td>[medium, should, have, warned, u, well, in, ad...</td>\n",
       "      <td>[medium, should, have, warn, u, well, in, adva...</td>\n",
       "      <td>medium should have warn u well in advance this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>arohaonces</td>\n",
       "      <td>i feel directly attacked  i consider moonbin  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, feel, directly, attacked, i, consider, moo...</td>\n",
       "      <td>[i, feel, directli, attack, i, consid, moonbin...</td>\n",
       "      <td>[(i, NN), (feel, VBP), (directly, RB), (attack...</td>\n",
       "      <td>[(i, n), (feel, v), (directly, r), (attacked, ...</td>\n",
       "      <td>[i, feel, directly, attacked, i, consider, moo...</td>\n",
       "      <td>[i, feel, directly, attack, i, consider, moonb...</td>\n",
       "      <td>i feel directly attack i consider moonbin jinj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>auroraborealis</td>\n",
       "      <td>ok who remember outcast nd the dora au those a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ok, who, remember, outcast, nd, the, dora, au...</td>\n",
       "      <td>[ok, who, rememb, outcast, nd, the, dora, au, ...</td>\n",
       "      <td>[(ok, NN), (who, WP), (remember, VBP), (outcas...</td>\n",
       "      <td>[(ok, n), (who, n), (remember, v), (outcast, n...</td>\n",
       "      <td>[ok, who, remember, outcast, nd, the, dora, au...</td>\n",
       "      <td>[ok, who, remember, outcast, nd, the, dora, au...</td>\n",
       "      <td>ok who remember outcast nd the dora au those a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11369</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jake corway wrecked while running 14th at irp</td>\n",
       "      <td>1</td>\n",
       "      <td>[jake, corway, wrecked, while, running, 14th, ...</td>\n",
       "      <td>[jake, corway, wreck, while, run, 14th, at, irp]</td>\n",
       "      <td>[(jake, NN), (corway, NN), (wrecked, VBD), (wh...</td>\n",
       "      <td>[(jake, n), (corway, n), (wrecked, v), (while,...</td>\n",
       "      <td>[jake, corway, wrecked, while, running, 14th, ...</td>\n",
       "      <td>[jake, corway, wreck, while, run, 14th, at, irp]</td>\n",
       "      <td>jake corway wreck while run 14th at irp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10898 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                                           location  \\\n",
       "id                                                                  \n",
       "0       ablaze                                                NaN   \n",
       "1       ablaze                                                NaN   \n",
       "2       ablaze                                      New York City   \n",
       "4       ablaze                                                NaN   \n",
       "5       ablaze                                                 OC   \n",
       "6       ablaze                                    London, England   \n",
       "7       ablaze                                             Bharat   \n",
       "8       ablaze                                       Accra, Ghana   \n",
       "9       ablaze                                          Searching   \n",
       "10      ablaze                                                NaN   \n",
       "11      ablaze                                                NaN   \n",
       "12      ablaze                                                NaN   \n",
       "13      ablaze                                          HYDERABAD   \n",
       "14      ablaze                                           Reno, NV   \n",
       "15      ablaze                                                NaN   \n",
       "16      ablaze                                                NaN   \n",
       "17      ablaze                                                NaN   \n",
       "18      ablaze                                          Worldwide   \n",
       "19      ablaze                                                NaN   \n",
       "20      ablaze                                                NaN   \n",
       "21      ablaze                                                NaN   \n",
       "22      ablaze                                              Italy   \n",
       "23      ablaze  ` ˗ˏˋ i'm⠀waiting⠀for⠀you⠀to⠀pour⠀my⠀𝘀𝗶𝗻𝘀⠀onto...   \n",
       "24      ablaze                                         Okielahoma   \n",
       "25      ablaze                                                NaN   \n",
       "26      ablaze                                                NaN   \n",
       "27      ablaze                                         Havana 3am   \n",
       "28      ablaze                                                NaN   \n",
       "29      ablaze                                                NaN   \n",
       "30      ablaze                                                NaN   \n",
       "...        ...                                                ...   \n",
       "11336  wrecked                                             𝕸𝖔𝖆𝖗𝖒𝖞   \n",
       "11337  wrecked                            Malilipot, Bicol Region   \n",
       "11338  wrecked                                            Puchong   \n",
       "11339  wrecked                                            fan acc   \n",
       "11340  wrecked                                  Oklahoma City, OK   \n",
       "11341  wrecked                                                NaN   \n",
       "11342  wrecked                                         Shropshire   \n",
       "11343  wrecked                                            EXO's 💜   \n",
       "11344  wrecked                                                 🇲🇾   \n",
       "11345  wrecked                                                NaN   \n",
       "11346  wrecked                                       Alabama, USA   \n",
       "11347  wrecked                                                NaN   \n",
       "11348  wrecked                                  Capon Bridge, WV,   \n",
       "11349  wrecked                                      Zion, Nigeria   \n",
       "11350  wrecked                                                NaN   \n",
       "11351  wrecked                                                NaN   \n",
       "11352  wrecked                                                NaN   \n",
       "11353  wrecked                                         magic shop   \n",
       "11354  wrecked                                                NaN   \n",
       "11355  wrecked                                             Recife   \n",
       "11358  wrecked                                    gguk's pocket ♡   \n",
       "11359  wrecked                                     Washington, DC   \n",
       "11360  wrecked                                              D(M)V   \n",
       "11362  wrecked                                   feuille d'érable   \n",
       "11363  wrecked                                                NaN   \n",
       "11364  wrecked                                                NaN   \n",
       "11365  wrecked                            Blue State in a red sea   \n",
       "11366  wrecked                                         arohaonces   \n",
       "11368  wrecked                                     auroraborealis   \n",
       "11369  wrecked                                                NaN   \n",
       "\n",
       "                                                    text  target  \\\n",
       "id                                                                 \n",
       "0      communal violence in bhainsa telangana stones ...       1   \n",
       "1      telangana section 144 has been imposed in bhai...       1   \n",
       "2                arsonist sets cars ablaze at dealership       1   \n",
       "4      lord jesus your love brings freedom and pardon...       0   \n",
       "5      if this child was chinese this tweet would hav...       0   \n",
       "6      several houses have been set ablaze in ngemsib...       1   \n",
       "7      asansol a bjp office in salanpur village was s...       1   \n",
       "8      national security minister kan dapaahs side ch...       0   \n",
       "9      this creature whos soul is no longer clarent b...       0   \n",
       "10     images showing the havoc caused by the cameroo...       1   \n",
       "11     social media went bananas after chuba hubbard ...       0   \n",
       "12     hausa youths set area office of apapaiganmu lo...       1   \n",
       "13     under mamatabanerjee political violence  vanda...       1   \n",
       "14                  amen set the whole system ablaze man       0   \n",
       "15     images showing the havoc caused by the cameroo...       1   \n",
       "16     no cows today but our local factory is sadly s...       1   \n",
       "17     rengoku sets my heart ablaze postscript i miss...       0   \n",
       "18     paulzizkaphoto rundle ablaze wishing you all a...       0   \n",
       "19     french cameroun set houses ablaze in ndu and r...       1   \n",
       "20     cameroons bir soldiers on the 05012020 invaded...       1   \n",
       "21     as fires ablaze throughout the landas the prop...       1   \n",
       "22     thankfultuesday isaiah 432 when you pass throu...       0   \n",
       "23     when you walk through the fire you will not be...       0   \n",
       "24     originally they were intended to be fired at b...       1   \n",
       "25     warm greetings to all on the occasion of lohri...       0   \n",
       "26     another arson in njikomboyonwr the ambazombies...       1   \n",
       "27     another public market in haiti mysteriously se...       1   \n",
       "28                               that is kind true sadly       0   \n",
       "29            i swear that jam will set the world ablaze       0   \n",
       "30     marivan kurdistan province monday jan 13th 202...       1   \n",
       "...                                                  ...     ...   \n",
       "11336   admit it we are all bias wrecked by soobin today       0   \n",
       "11337  hi so ive started to stan the girl group ans b...       0   \n",
       "11338  kesian ular we have wrecked their natural habitat       1   \n",
       "11339  my twat of an art teacher  what grades do you ...       0   \n",
       "11340  my first listen was also in the whip i damn ne...       0   \n",
       "11341  hes not a journalist and doesnt pretend to be ...       0   \n",
       "11342  by all means give up sugar and carbs your body...       0   \n",
       "11343  chanyeol 2k19 gaming destroys me messed my min...       0   \n",
       "11344                    hell be wrecked if that happens       0   \n",
       "11345  hes the oxygen that pumps blood to my living h...       0   \n",
       "11346  when youre watching clemson get wrecked and se...       0   \n",
       "11347  why would operators put new buses on school co...       0   \n",
       "11348  democratic propaganda wrecked libya obama just...       0   \n",
       "11349   cc this is what i was telling you the other time       0   \n",
       "11350  tryna think of a plot but nothing comes to min...       0   \n",
       "11351               this was me when my car got wrecked        0   \n",
       "11352  since everyone is talking about chen and exo o...       0   \n",
       "11353  is it possible to be bias wrecked by your own ...       0   \n",
       "11354  yeah proper liverpool fans wrecked man citys b...       1   \n",
       "11355  trump and sisi rejected foreign exploitation a...       1   \n",
       "11358  i get wrecked too shake my head  my biases in ...       0   \n",
       "11359  trump and sisi rejected foreign exploitation a...       1   \n",
       "11360  man gogo version of gucci bandana just came on...       0   \n",
       "11362  stell wrecked ako palagi sayo haha alabtopspot...       0   \n",
       "11363  hes the oxygen that pumps blood to my living h...       0   \n",
       "11364  had these guys last game n fcked them talked n...       0   \n",
       "11365  media should have warned us well in advance th...       0   \n",
       "11366  i feel directly attacked  i consider moonbin  ...       0   \n",
       "11368  ok who remember outcast nd the dora au those a...       0   \n",
       "11369      jake corway wrecked while running 14th at irp       1   \n",
       "\n",
       "                                               tokenized  \\\n",
       "id                                                         \n",
       "0      [communal, violence, in, bhainsa, telangana, s...   \n",
       "1      [telangana, section, 144, has, been, imposed, ...   \n",
       "2         [arsonist, sets, cars, ablaze, at, dealership]   \n",
       "4      [lord, jesus, your, love, brings, freedom, and...   \n",
       "5      [if, this, child, was, chinese, this, tweet, w...   \n",
       "6      [several, houses, have, been, set, ablaze, in,...   \n",
       "7      [asansol, a, bjp, office, in, salanpur, villag...   \n",
       "8      [national, security, minister, kan, dapaahs, s...   \n",
       "9      [this, creature, whos, soul, is, no, longer, c...   \n",
       "10     [images, showing, the, havoc, caused, by, the,...   \n",
       "11     [social, media, went, bananas, after, chuba, h...   \n",
       "12     [hausa, youths, set, area, office, of, apapaig...   \n",
       "13     [under, mamatabanerjee, political, violence, v...   \n",
       "14          [amen, set, the, whole, system, ablaze, man]   \n",
       "15     [images, showing, the, havoc, caused, by, the,...   \n",
       "16     [no, cows, today, but, our, local, factory, is...   \n",
       "17     [rengoku, sets, my, heart, ablaze, postscript,...   \n",
       "18     [paulzizkaphoto, rundle, ablaze, wishing, you,...   \n",
       "19     [french, cameroun, set, houses, ablaze, in, nd...   \n",
       "20     [cameroons, bir, soldiers, on, the, 05012020, ...   \n",
       "21     [as, fires, ablaze, throughout, the, landas, t...   \n",
       "22     [thankfultuesday, isaiah, 432, when, you, pass...   \n",
       "23     [when, you, walk, through, the, fire, you, wil...   \n",
       "24     [originally, they, were, intended, to, be, fir...   \n",
       "25     [warm, greetings, to, all, on, the, occasion, ...   \n",
       "26     [another, arson, in, njikomboyonwr, the, ambaz...   \n",
       "27     [another, public, market, in, haiti, mysteriou...   \n",
       "28                         [that, is, kind, true, sadly]   \n",
       "29     [i, swear, that, jam, will, set, the, world, a...   \n",
       "30     [marivan, kurdistan, province, monday, jan, 13...   \n",
       "...                                                  ...   \n",
       "11336  [admit, it, we, are, all, bias, wrecked, by, s...   \n",
       "11337  [hi, so, ive, started, to, stan, the, girl, gr...   \n",
       "11338  [kesian, ular, we, have, wrecked, their, natur...   \n",
       "11339  [my, twat, of, an, art, teacher, what, grades,...   \n",
       "11340  [my, first, listen, was, also, in, the, whip, ...   \n",
       "11341  [hes, not, a, journalist, and, doesnt, pretend...   \n",
       "11342  [by, all, means, give, up, sugar, and, carbs, ...   \n",
       "11343  [chanyeol, 2k19, gaming, destroys, me, messed,...   \n",
       "11344             [hell, be, wrecked, if, that, happens]   \n",
       "11345  [hes, the, oxygen, that, pumps, blood, to, my,...   \n",
       "11346  [when, youre, watching, clemson, get, wrecked,...   \n",
       "11347  [why, would, operators, put, new, buses, on, s...   \n",
       "11348  [democratic, propaganda, wrecked, libya, obama...   \n",
       "11349  [cc, this, is, what, i, was, telling, you, the...   \n",
       "11350  [tryna, think, of, a, plot, but, nothing, come...   \n",
       "11351       [this, was, me, when, my, car, got, wrecked]   \n",
       "11352  [since, everyone, is, talking, about, chen, an...   \n",
       "11353  [is, it, possible, to, be, bias, wrecked, by, ...   \n",
       "11354  [yeah, proper, liverpool, fans, wrecked, man, ...   \n",
       "11355  [trump, and, sisi, rejected, foreign, exploita...   \n",
       "11358  [i, get, wrecked, too, shake, my, head, my, bi...   \n",
       "11359  [trump, and, sisi, rejected, foreign, exploita...   \n",
       "11360  [man, gogo, version, of, gucci, bandana, just,...   \n",
       "11362  [stell, wrecked, ako, palagi, sayo, haha, alab...   \n",
       "11363  [hes, the, oxygen, that, pumps, blood, to, my,...   \n",
       "11364  [had, these, guys, last, game, n, fcked, them,...   \n",
       "11365  [media, should, have, warned, us, well, in, ad...   \n",
       "11366  [i, feel, directly, attacked, i, consider, moo...   \n",
       "11368  [ok, who, remember, outcast, nd, the, dora, au...   \n",
       "11369  [jake, corway, wrecked, while, running, 14th, ...   \n",
       "\n",
       "                                           PorterStemmer  \\\n",
       "id                                                         \n",
       "0      [commun, violenc, in, bhainsa, telangana, ston...   \n",
       "1      [telangana, section, 144, ha, been, impos, in,...   \n",
       "2            [arsonist, set, car, ablaz, at, dealership]   \n",
       "4      [lord, jesu, your, love, bring, freedom, and, ...   \n",
       "5      [if, thi, child, wa, chines, thi, tweet, would...   \n",
       "6      [sever, hous, have, been, set, ablaz, in, ngem...   \n",
       "7      [asansol, a, bjp, offic, in, salanpur, villag,...   \n",
       "8      [nation, secur, minist, kan, dapaah, side, chi...   \n",
       "9      [thi, creatur, who, soul, is, no, longer, clar...   \n",
       "10     [imag, show, the, havoc, caus, by, the, camero...   \n",
       "11     [social, media, went, banana, after, chuba, hu...   \n",
       "12     [hausa, youth, set, area, offic, of, apapaigan...   \n",
       "13     [under, mamatabanerje, polit, violenc, vandal,...   \n",
       "14           [amen, set, the, whole, system, ablaz, man]   \n",
       "15     [imag, show, the, havoc, caus, by, the, camero...   \n",
       "16     [no, cow, today, but, our, local, factori, is,...   \n",
       "17     [rengoku, set, my, heart, ablaz, postscript, i...   \n",
       "18     [paulzizkaphoto, rundl, ablaz, wish, you, all,...   \n",
       "19     [french, cameroun, set, hous, ablaz, in, ndu, ...   \n",
       "20     [cameroon, bir, soldier, on, the, 05012020, in...   \n",
       "21     [as, fire, ablaz, throughout, the, landa, the,...   \n",
       "22     [thankfultuesday, isaiah, 432, when, you, pass...   \n",
       "23     [when, you, walk, through, the, fire, you, wil...   \n",
       "24     [origin, they, were, intend, to, be, fire, at,...   \n",
       "25     [warm, greet, to, all, on, the, occas, of, loh...   \n",
       "26     [anoth, arson, in, njikomboyonwr, the, ambazom...   \n",
       "27     [anoth, public, market, in, haiti, mysteri, se...   \n",
       "28                         [that, is, kind, true, sadli]   \n",
       "29     [i, swear, that, jam, will, set, the, world, a...   \n",
       "30     [marivan, kurdistan, provinc, monday, jan, 13t...   \n",
       "...                                                  ...   \n",
       "11336  [admit, it, we, are, all, bia, wreck, by, soob...   \n",
       "11337  [hi, so, ive, start, to, stan, the, girl, grou...   \n",
       "11338  [kesian, ular, we, have, wreck, their, natur, ...   \n",
       "11339  [my, twat, of, an, art, teacher, what, grade, ...   \n",
       "11340  [my, first, listen, wa, also, in, the, whip, i...   \n",
       "11341  [he, not, a, journalist, and, doesnt, pretend,...   \n",
       "11342  [by, all, mean, give, up, sugar, and, carb, yo...   \n",
       "11343  [chanyeol, 2k19, game, destroy, me, mess, my, ...   \n",
       "11344                [hell, be, wreck, if, that, happen]   \n",
       "11345  [he, the, oxygen, that, pump, blood, to, my, l...   \n",
       "11346  [when, your, watch, clemson, get, wreck, and, ...   \n",
       "11347  [whi, would, oper, put, new, buse, on, school,...   \n",
       "11348  [democrat, propaganda, wreck, libya, obama, ju...   \n",
       "11349  [cc, thi, is, what, i, wa, tell, you, the, oth...   \n",
       "11350  [tryna, think, of, a, plot, but, noth, come, t...   \n",
       "11351           [thi, wa, me, when, my, car, got, wreck]   \n",
       "11352  [sinc, everyon, is, talk, about, chen, and, ex...   \n",
       "11353  [is, it, possibl, to, be, bia, wreck, by, your...   \n",
       "11354  [yeah, proper, liverpool, fan, wreck, man, cit...   \n",
       "11355  [trump, and, sisi, reject, foreign, exploit, a...   \n",
       "11358  [i, get, wreck, too, shake, my, head, my, bias...   \n",
       "11359  [trump, and, sisi, reject, foreign, exploit, a...   \n",
       "11360  [man, gogo, version, of, gucci, bandana, just,...   \n",
       "11362  [stell, wreck, ako, palagi, sayo, haha, alabto...   \n",
       "11363  [he, the, oxygen, that, pump, blood, to, my, l...   \n",
       "11364  [had, these, guy, last, game, n, fcked, them, ...   \n",
       "11365  [media, should, have, warn, us, well, in, adva...   \n",
       "11366  [i, feel, directli, attack, i, consid, moonbin...   \n",
       "11368  [ok, who, rememb, outcast, nd, the, dora, au, ...   \n",
       "11369   [jake, corway, wreck, while, run, 14th, at, irp]   \n",
       "\n",
       "                                          defualt_postag  \\\n",
       "id                                                         \n",
       "0      [(communal, JJ), (violence, NN), (in, IN), (bh...   \n",
       "1      [(telangana, JJ), (section, NN), (144, CD), (h...   \n",
       "2      [(arsonist, JJ), (sets, NNS), (cars, NNS), (ab...   \n",
       "4      [(lord, NN), (jesus, VBZ), (your, PRP$), (love...   \n",
       "5      [(if, IN), (this, DT), (child, NN), (was, VBD)...   \n",
       "6      [(several, JJ), (houses, NNS), (have, VBP), (b...   \n",
       "7      [(asansol, VB), (a, DT), (bjp, NN), (office, N...   \n",
       "8      [(national, JJ), (security, NN), (minister, NN...   \n",
       "9      [(this, DT), (creature, NN), (whos, JJ), (soul...   \n",
       "10     [(images, NNS), (showing, VBG), (the, DT), (ha...   \n",
       "11     [(social, JJ), (media, NNS), (went, VBD), (ban...   \n",
       "12     [(hausa, JJ), (youths, NNS), (set, VBN), (area...   \n",
       "13     [(under, IN), (mamatabanerjee, JJ), (political...   \n",
       "14     [(amen, NNS), (set, VBD), (the, DT), (whole, J...   \n",
       "15     [(images, NNS), (showing, VBG), (the, DT), (ha...   \n",
       "16     [(no, DT), (cows, NNS), (today, NN), (but, CC)...   \n",
       "17     [(rengoku, NN), (sets, NNS), (my, PRP$), (hear...   \n",
       "18     [(paulzizkaphoto, NN), (rundle, NN), (ablaze, ...   \n",
       "19     [(french, JJ), (cameroun, NN), (set, VBN), (ho...   \n",
       "20     [(cameroons, NNS), (bir, VBP), (soldiers, NNS)...   \n",
       "21     [(as, IN), (fires, NNS), (ablaze, VBP), (throu...   \n",
       "22     [(thankfultuesday, NN), (isaiah, VBZ), (432, C...   \n",
       "23     [(when, WRB), (you, PRP), (walk, VBP), (throug...   \n",
       "24     [(originally, RB), (they, PRP), (were, VBD), (...   \n",
       "25     [(warm, JJ), (greetings, NNS), (to, TO), (all,...   \n",
       "26     [(another, DT), (arson, NN), (in, IN), (njikom...   \n",
       "27     [(another, DT), (public, JJ), (market, NN), (i...   \n",
       "28     [(that, DT), (is, VBZ), (kind, NN), (true, JJ)...   \n",
       "29     [(i, NN), (swear, VBP), (that, IN), (jam, NN),...   \n",
       "30     [(marivan, NN), (kurdistan, NN), (province, NN...   \n",
       "...                                                  ...   \n",
       "11336  [(admit, NN), (it, PRP), (we, PRP), (are, VBP)...   \n",
       "11337  [(hi, NNS), (so, RB), (ive, JJ), (started, VBD...   \n",
       "11338  [(kesian, JJ), (ular, NN), (we, PRP), (have, V...   \n",
       "11339  [(my, PRP$), (twat, NN), (of, IN), (an, DT), (...   \n",
       "11340  [(my, PRP$), (first, JJ), (listen, NN), (was, ...   \n",
       "11341  [(hes, NNS), (not, RB), (a, DT), (journalist, ...   \n",
       "11342  [(by, IN), (all, DT), (means, NNS), (give, VBP...   \n",
       "11343  [(chanyeol, NN), (2k19, CD), (gaming, NN), (de...   \n",
       "11344  [(hell, NN), (be, VB), (wrecked, VBN), (if, IN...   \n",
       "11345  [(hes, NNS), (the, DT), (oxygen, NN), (that, W...   \n",
       "11346  [(when, WRB), (youre, NN), (watching, VBG), (c...   \n",
       "11347  [(why, WRB), (would, MD), (operators, NNS), (p...   \n",
       "11348  [(democratic, JJ), (propaganda, NN), (wrecked,...   \n",
       "11349  [(cc, NN), (this, DT), (is, VBZ), (what, WP), ...   \n",
       "11350  [(tryna, JJ), (think, NN), (of, IN), (a, DT), ...   \n",
       "11351  [(this, DT), (was, VBD), (me, PRP), (when, WRB...   \n",
       "11352  [(since, IN), (everyone, NN), (is, VBZ), (talk...   \n",
       "11353  [(is, VBZ), (it, PRP), (possible, JJ), (to, TO...   \n",
       "11354  [(yeah, UH), (proper, JJ), (liverpool, NN), (f...   \n",
       "11355  [(trump, NN), (and, CC), (sisi, NN), (rejected...   \n",
       "11358  [(i, NN), (get, VBP), (wrecked, VBN), (too, RB...   \n",
       "11359  [(trump, NN), (and, CC), (sisi, NN), (rejected...   \n",
       "11360  [(man, NN), (gogo, VBZ), (version, NN), (of, I...   \n",
       "11362  [(stell, NN), (wrecked, VBD), (ako, JJ), (pala...   \n",
       "11363  [(hes, NNS), (the, DT), (oxygen, NN), (that, W...   \n",
       "11364  [(had, VBD), (these, DT), (guys, NNS), (last, ...   \n",
       "11365  [(media, NNS), (should, MD), (have, VB), (warn...   \n",
       "11366  [(i, NN), (feel, VBP), (directly, RB), (attack...   \n",
       "11368  [(ok, NN), (who, WP), (remember, VBP), (outcas...   \n",
       "11369  [(jake, NN), (corway, NN), (wrecked, VBD), (wh...   \n",
       "\n",
       "                                    combined_postag_wnet  \\\n",
       "id                                                         \n",
       "0      [(communal, n), (violence, n), (in, n), (bhain...   \n",
       "1      [(telangana, n), (section, n), (144, n), (has,...   \n",
       "2      [(arsonist, n), (sets, v), (cars, n), (ablaze,...   \n",
       "4      [(lord, n), (jesus, n), (your, n), (love, v), ...   \n",
       "5      [(if, n), (this, n), (child, n), (was, n), (ch...   \n",
       "6      [(several, n), (houses, n), (have, n), (been, ...   \n",
       "7      [(asansol, n), (a, n), (bjp, n), (office, n), ...   \n",
       "8      [(national, a), (security, n), (minister, n), ...   \n",
       "9      [(this, n), (creature, n), (whos, n), (soul, n...   \n",
       "10     [(images, n), (showing, v), (the, n), (havoc, ...   \n",
       "11     [(social, a), (media, n), (went, v), (bananas,...   \n",
       "12     [(hausa, n), (youths, n), (set, v), (area, n),...   \n",
       "13     [(under, n), (mamatabanerjee, n), (political, ...   \n",
       "14     [(amen, n), (set, v), (the, n), (whole, a), (s...   \n",
       "15     [(images, n), (showing, v), (the, n), (havoc, ...   \n",
       "16     [(no, n), (cows, n), (today, n), (but, n), (ou...   \n",
       "17     [(rengoku, n), (sets, v), (my, n), (heart, n),...   \n",
       "18     [(paulzizkaphoto, n), (rundle, n), (ablaze, r)...   \n",
       "19     [(french, n), (cameroun, n), (set, v), (houses...   \n",
       "20     [(cameroons, n), (bir, n), (soldiers, n), (on,...   \n",
       "21     [(as, n), (fires, n), (ablaze, r), (throughout...   \n",
       "22     [(thankfultuesday, n), (isaiah, n), (432, n), ...   \n",
       "23     [(when, n), (you, n), (walk, v), (through, n),...   \n",
       "24     [(originally, r), (they, n), (were, n), (inten...   \n",
       "25     [(warm, a), (greetings, n), (to, n), (all, n),...   \n",
       "26     [(another, n), (arson, n), (in, n), (njikomboy...   \n",
       "27     [(another, n), (public, a), (market, n), (in, ...   \n",
       "28     [(that, n), (is, n), (kind, n), (true, a), (sa...   \n",
       "29     [(i, n), (swear, n), (that, n), (jam, n), (wil...   \n",
       "30     [(marivan, n), (kurdistan, n), (province, n), ...   \n",
       "...                                                  ...   \n",
       "11336  [(admit, v), (it, n), (we, n), (are, n), (all,...   \n",
       "11337  [(hi, n), (so, n), (ive, n), (started, v), (to...   \n",
       "11338  [(kesian, n), (ular, n), (we, n), (have, n), (...   \n",
       "11339  [(my, n), (twat, n), (of, n), (an, n), (art, n...   \n",
       "11340  [(my, n), (first, n), (listen, v), (was, n), (...   \n",
       "11341  [(hes, n), (not, n), (a, n), (journalist, n), ...   \n",
       "11342  [(by, n), (all, n), (means, n), (give, v), (up...   \n",
       "11343  [(chanyeol, n), (2k19, n), (gaming, n), (destr...   \n",
       "11344  [(hell, n), (be, n), (wrecked, v), (if, n), (t...   \n",
       "11345  [(hes, n), (the, n), (oxygen, n), (that, n), (...   \n",
       "11346  [(when, n), (youre, n), (watching, v), (clemso...   \n",
       "11347  [(why, n), (would, n), (operators, n), (put, v...   \n",
       "11348  [(democratic, a), (propaganda, n), (wrecked, v...   \n",
       "11349  [(cc, n), (this, n), (is, n), (what, n), (i, n...   \n",
       "11350  [(tryna, n), (think, v), (of, n), (a, n), (plo...   \n",
       "11351  [(this, n), (was, n), (me, n), (when, n), (my,...   \n",
       "11352  [(since, n), (everyone, n), (is, n), (talking,...   \n",
       "11353  [(is, n), (it, n), (possible, a), (to, n), (be...   \n",
       "11354  [(yeah, n), (proper, a), (liverpool, n), (fans...   \n",
       "11355  [(trump, n), (and, n), (sisi, n), (rejected, v...   \n",
       "11358  [(i, n), (get, v), (wrecked, v), (too, n), (sh...   \n",
       "11359  [(trump, n), (and, n), (sisi, n), (rejected, v...   \n",
       "11360  [(man, n), (gogo, n), (version, n), (of, n), (...   \n",
       "11362  [(stell, n), (wrecked, v), (ako, n), (palagi, ...   \n",
       "11363  [(hes, n), (the, n), (oxygen, n), (that, n), (...   \n",
       "11364  [(had, n), (these, n), (guys, n), (last, n), (...   \n",
       "11365  [(media, n), (should, n), (have, n), (warned, ...   \n",
       "11366  [(i, n), (feel, v), (directly, r), (attacked, ...   \n",
       "11368  [(ok, n), (who, n), (remember, v), (outcast, n...   \n",
       "11369  [(jake, n), (corway, n), (wrecked, v), (while,...   \n",
       "\n",
       "                             lemmatized_word_without_pos  \\\n",
       "id                                                         \n",
       "0      [communal, violence, in, bhainsa, telangana, s...   \n",
       "1      [telangana, section, 144, ha, been, imposed, i...   \n",
       "2           [arsonist, set, car, ablaze, at, dealership]   \n",
       "4      [lord, jesus, your, love, brings, freedom, and...   \n",
       "5      [if, this, child, wa, chinese, this, tweet, wo...   \n",
       "6      [several, house, have, been, set, ablaze, in, ...   \n",
       "7      [asansol, a, bjp, office, in, salanpur, villag...   \n",
       "8      [national, security, minister, kan, dapaahs, s...   \n",
       "9      [this, creature, who, soul, is, no, longer, cl...   \n",
       "10     [image, showing, the, havoc, caused, by, the, ...   \n",
       "11     [social, medium, went, banana, after, chuba, h...   \n",
       "12     [hausa, youth, set, area, office, of, apapaiga...   \n",
       "13     [under, mamatabanerjee, political, violence, v...   \n",
       "14          [amen, set, the, whole, system, ablaze, man]   \n",
       "15     [image, showing, the, havoc, caused, by, the, ...   \n",
       "16     [no, cow, today, but, our, local, factory, is,...   \n",
       "17     [rengoku, set, my, heart, ablaze, postscript, ...   \n",
       "18     [paulzizkaphoto, rundle, ablaze, wishing, you,...   \n",
       "19     [french, cameroun, set, house, ablaze, in, ndu...   \n",
       "20     [cameroon, bir, soldier, on, the, 05012020, in...   \n",
       "21     [a, fire, ablaze, throughout, the, landas, the...   \n",
       "22     [thankfultuesday, isaiah, 432, when, you, pas,...   \n",
       "23     [when, you, walk, through, the, fire, you, wil...   \n",
       "24     [originally, they, were, intended, to, be, fir...   \n",
       "25     [warm, greeting, to, all, on, the, occasion, o...   \n",
       "26     [another, arson, in, njikomboyonwr, the, ambaz...   \n",
       "27     [another, public, market, in, haiti, mysteriou...   \n",
       "28                         [that, is, kind, true, sadly]   \n",
       "29     [i, swear, that, jam, will, set, the, world, a...   \n",
       "30     [marivan, kurdistan, province, monday, jan, 13...   \n",
       "...                                                  ...   \n",
       "11336  [admit, it, we, are, all, bias, wrecked, by, s...   \n",
       "11337  [hi, so, ive, started, to, stan, the, girl, gr...   \n",
       "11338  [kesian, ular, we, have, wrecked, their, natur...   \n",
       "11339  [my, twat, of, an, art, teacher, what, grade, ...   \n",
       "11340  [my, first, listen, wa, also, in, the, whip, i...   \n",
       "11341  [he, not, a, journalist, and, doesnt, pretend,...   \n",
       "11342  [by, all, mean, give, up, sugar, and, carbs, y...   \n",
       "11343  [chanyeol, 2k19, gaming, destroys, me, messed,...   \n",
       "11344             [hell, be, wrecked, if, that, happens]   \n",
       "11345  [he, the, oxygen, that, pump, blood, to, my, l...   \n",
       "11346  [when, youre, watching, clemson, get, wrecked,...   \n",
       "11347  [why, would, operator, put, new, bus, on, scho...   \n",
       "11348  [democratic, propaganda, wrecked, libya, obama...   \n",
       "11349  [cc, this, is, what, i, wa, telling, you, the,...   \n",
       "11350  [tryna, think, of, a, plot, but, nothing, come...   \n",
       "11351        [this, wa, me, when, my, car, got, wrecked]   \n",
       "11352  [since, everyone, is, talking, about, chen, an...   \n",
       "11353  [is, it, possible, to, be, bias, wrecked, by, ...   \n",
       "11354  [yeah, proper, liverpool, fan, wrecked, man, c...   \n",
       "11355  [trump, and, sisi, rejected, foreign, exploita...   \n",
       "11358  [i, get, wrecked, too, shake, my, head, my, bi...   \n",
       "11359  [trump, and, sisi, rejected, foreign, exploita...   \n",
       "11360  [man, gogo, version, of, gucci, bandana, just,...   \n",
       "11362  [stell, wrecked, ako, palagi, sayo, haha, alab...   \n",
       "11363  [he, the, oxygen, that, pump, blood, to, my, l...   \n",
       "11364  [had, these, guy, last, game, n, fcked, them, ...   \n",
       "11365  [medium, should, have, warned, u, well, in, ad...   \n",
       "11366  [i, feel, directly, attacked, i, consider, moo...   \n",
       "11368  [ok, who, remember, outcast, nd, the, dora, au...   \n",
       "11369  [jake, corway, wrecked, while, running, 14th, ...   \n",
       "\n",
       "                                lemmatized_word_with_pos  \\\n",
       "id                                                         \n",
       "0      [communal, violence, in, bhainsa, telangana, s...   \n",
       "1      [telangana, section, 144, ha, been, impose, in...   \n",
       "2           [arsonist, set, car, ablaze, at, dealership]   \n",
       "4      [lord, jesus, your, love, bring, freedom, and,...   \n",
       "5      [if, this, child, wa, chinese, this, tweet, wo...   \n",
       "6      [several, house, have, been, set, ablaze, in, ...   \n",
       "7      [asansol, a, bjp, office, in, salanpur, villag...   \n",
       "8      [national, security, minister, kan, dapaahs, s...   \n",
       "9      [this, creature, who, soul, is, no, longer, cl...   \n",
       "10     [image, show, the, havoc, cause, by, the, came...   \n",
       "11     [social, medium, go, banana, after, chuba, hub...   \n",
       "12     [hausa, youth, set, area, office, of, apapaiga...   \n",
       "13     [under, mamatabanerjee, political, violence, v...   \n",
       "14          [amen, set, the, whole, system, ablaze, man]   \n",
       "15     [image, show, the, havoc, cause, by, the, came...   \n",
       "16     [no, cow, today, but, our, local, factory, is,...   \n",
       "17     [rengoku, set, my, heart, ablaze, postscript, ...   \n",
       "18     [paulzizkaphoto, rundle, ablaze, wishing, you,...   \n",
       "19     [french, cameroun, set, house, ablaze, in, ndu...   \n",
       "20     [cameroon, bir, soldier, on, the, 05012020, in...   \n",
       "21     [a, fire, ablaze, throughout, the, landas, the...   \n",
       "22     [thankfultuesday, isaiah, 432, when, you, pass...   \n",
       "23     [when, you, walk, through, the, fire, you, wil...   \n",
       "24     [originally, they, were, intend, to, be, fire,...   \n",
       "25     [warm, greeting, to, all, on, the, occasion, o...   \n",
       "26     [another, arson, in, njikomboyonwr, the, ambaz...   \n",
       "27     [another, public, market, in, haiti, mysteriou...   \n",
       "28                         [that, is, kind, true, sadly]   \n",
       "29     [i, swear, that, jam, will, set, the, world, a...   \n",
       "30     [marivan, kurdistan, province, monday, jan, 13...   \n",
       "...                                                  ...   \n",
       "11336  [admit, it, we, are, all, bias, wreck, by, soo...   \n",
       "11337  [hi, so, ive, start, to, stan, the, girl, grou...   \n",
       "11338  [kesian, ular, we, have, wreck, their, natural...   \n",
       "11339  [my, twat, of, an, art, teacher, what, grade, ...   \n",
       "11340  [my, first, listen, wa, also, in, the, whip, i...   \n",
       "11341  [he, not, a, journalist, and, doesnt, pretend,...   \n",
       "11342  [by, all, mean, give, up, sugar, and, carbs, y...   \n",
       "11343  [chanyeol, 2k19, gaming, destroys, me, messed,...   \n",
       "11344                [hell, be, wreck, if, that, happen]   \n",
       "11345  [he, the, oxygen, that, pump, blood, to, my, l...   \n",
       "11346  [when, youre, watch, clemson, get, wreck, and,...   \n",
       "11347  [why, would, operator, put, new, bus, on, scho...   \n",
       "11348  [democratic, propaganda, wreck, libya, obama, ...   \n",
       "11349  [cc, this, is, what, i, wa, tell, you, the, ot...   \n",
       "11350  [tryna, think, of, a, plot, but, nothing, come...   \n",
       "11351          [this, wa, me, when, my, car, get, wreck]   \n",
       "11352  [since, everyone, is, talk, about, chen, and, ...   \n",
       "11353  [is, it, possible, to, be, bias, wreck, by, yo...   \n",
       "11354  [yeah, proper, liverpool, fan, wreck, man, cit...   \n",
       "11355  [trump, and, sisi, reject, foreign, exploitati...   \n",
       "11358  [i, get, wreck, too, shake, my, head, my, bias...   \n",
       "11359  [trump, and, sisi, reject, foreign, exploitati...   \n",
       "11360  [man, gogo, version, of, gucci, bandana, just,...   \n",
       "11362  [stell, wreck, ako, palagi, sayo, haha, alabto...   \n",
       "11363  [he, the, oxygen, that, pump, blood, to, my, l...   \n",
       "11364  [had, these, guy, last, game, n, fcked, them, ...   \n",
       "11365  [medium, should, have, warn, u, well, in, adva...   \n",
       "11366  [i, feel, directly, attack, i, consider, moonb...   \n",
       "11368  [ok, who, remember, outcast, nd, the, dora, au...   \n",
       "11369   [jake, corway, wreck, while, run, 14th, at, irp]   \n",
       "\n",
       "                                         lemmatized_text  \n",
       "id                                                        \n",
       "0      communal violence in bhainsa telangana stone w...  \n",
       "1      telangana section 144 ha been impose in bhains...  \n",
       "2                  arsonist set car ablaze at dealership  \n",
       "4      lord jesus your love bring freedom and pardon ...  \n",
       "5      if this child wa chinese this tweet would have...  \n",
       "6      several house have been set ablaze in ngemsiba...  \n",
       "7      asansol a bjp office in salanpur village wa se...  \n",
       "8      national security minister kan dapaahs side ch...  \n",
       "9      this creature who soul is no longer clarent bu...  \n",
       "10     image show the havoc cause by the cameroon mil...  \n",
       "11     social medium go banana after chuba hubbard an...  \n",
       "12     hausa youth set area office of apapaiganmu loc...  \n",
       "13     under mamatabanerjee political violence vandal...  \n",
       "14                  amen set the whole system ablaze man  \n",
       "15     image show the havoc cause by the cameroon mil...  \n",
       "16     no cow today but our local factory is sadly st...  \n",
       "17     rengoku set my heart ablaze postscript i miss ...  \n",
       "18     paulzizkaphoto rundle ablaze wishing you all a...  \n",
       "19     french cameroun set house ablaze in ndu and ro...  \n",
       "20     cameroon bir soldier on the 05012020 invade th...  \n",
       "21     a fire ablaze throughout the landas the propho...  \n",
       "22     thankfultuesday isaiah 432 when you pass throu...  \n",
       "23     when you walk through the fire you will not be...  \n",
       "24     originally they were intend to be fire at boar...  \n",
       "25     warm greeting to all on the occasion of lohri ...  \n",
       "26     another arson in njikomboyonwr the ambazombies...  \n",
       "27     another public market in haiti mysteriously se...  \n",
       "28                               that is kind true sadly  \n",
       "29            i swear that jam will set the world ablaze  \n",
       "30     marivan kurdistan province monday jan 13th 202...  \n",
       "...                                                  ...  \n",
       "11336     admit it we are all bias wreck by soobin today  \n",
       "11337  hi so ive start to stan the girl group an beca...  \n",
       "11338    kesian ular we have wreck their natural habitat  \n",
       "11339  my twat of an art teacher what grade do you ne...  \n",
       "11340  my first listen wa also in the whip i damn nea...  \n",
       "11341  he not a journalist and doesnt pretend to be h...  \n",
       "11342  by all mean give up sugar and carbs your body ...  \n",
       "11343  chanyeol 2k19 gaming destroys me messed my min...  \n",
       "11344                       hell be wreck if that happen  \n",
       "11345  he the oxygen that pump blood to my live heart...  \n",
       "11346  when youre watch clemson get wreck and see the...  \n",
       "11347  why would operator put new bus on school contr...  \n",
       "11348  democratic propaganda wreck libya obama just h...  \n",
       "11349       cc this is what i wa tell you the other time  \n",
       "11350  tryna think of a plot but nothing come to mind...  \n",
       "11351                   this wa me when my car get wreck  \n",
       "11352  since everyone is talk about chen and exo on m...  \n",
       "11353   is it possible to be bias wreck by your own bias  \n",
       "11354  yeah proper liverpool fan wreck man city bus a...  \n",
       "11355  trump and sisi reject foreign exploitation and...  \n",
       "11358  i get wreck too shake my head my bias in skz a...  \n",
       "11359  trump and sisi reject foreign exploitation and...  \n",
       "11360  man gogo version of gucci bandana just come on...  \n",
       "11362  stell wreck ako palagi sayo haha alabtopspoton...  \n",
       "11363  he the oxygen that pump blood to my live heart...  \n",
       "11364  had these guy last game n fcked them talk nons...  \n",
       "11365  medium should have warn u well in advance this...  \n",
       "11366  i feel directly attack i consider moonbin jinj...  \n",
       "11368  ok who remember outcast nd the dora au those a...  \n",
       "11369            jake corway wreck while run 14th at irp  \n",
       "\n",
       "[10898 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words:\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# CountVectorizer(ngram_range=(1,ngram),stop_words='english')\n",
    "\n",
    "# or\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('English')\n",
    "# CountVectorizer(ngram_range=(1,ngram),stop_words=stop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def bag_of_words (data, ngram=1):\n",
    "    vect=CountVectorizer(ngram_range=(1,ngram),stop_words='english')\n",
    "    bags=vect.fit_transform(data)\n",
    "    return bags, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf(data, ngram=1):\n",
    "    vect=TfidfVectorizer(ngram_range=(1,ngram),stop_words='english',min_df=5,)\n",
    "    bags=vect.fit_transform(data)\n",
    "    return bags, vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BagofWords=bag_of_words(df['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10898, 19269)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BagofWords[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf=tfidf(df['lemmatized_text'],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10898, 3879)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non_Disaster tweets % :82.20\n"
     ]
    }
   ],
   "source": [
    "xs=df['target'].value_counts().index\n",
    "ys=df['target'].value_counts().values\n",
    "plt.figure()\n",
    "plt.bar(xs,ys,\n",
    "        color=['navy','darkgreen'],alpha=.6,width=.4)\n",
    "\n",
    "plt.gca().axes.get_yaxis().set_ticks([])\n",
    "plt.gca().axes.get_xaxis().set_ticks([0,1])\n",
    "# plt.gca().axes.get_ticklabels(['non_disaster','disaster'])\n",
    "\n",
    "plt.gca().set_facecolor('white')\n",
    "\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "#plt.gca().spines['bottom'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "for x,y in zip(xs,ys):\n",
    "\n",
    "    label = \"{}\".format(y)\n",
    "\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or \n",
    "plt.grid(False)\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('Tweets')\n",
    "\n",
    "plt.show()\n",
    "#print(len(df))\n",
    "print('Non_Disaster tweets % :{:.2f}' .format((df['target'].value_counts()[0]/len(df))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=df_tfidf[0]\n",
    "y=df['target']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10898, 3879)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train set: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "print('Accuracy of train set: {:.2f}' .format(clf.score(X_train,y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test set: 0.88\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of test set: {:.2f}' .format(clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93      2251\n",
      "           1       0.86      0.39      0.54       474\n",
      "\n",
      "    accuracy                           0.88      2725\n",
      "   macro avg       0.87      0.69      0.73      2725\n",
      "weighted avg       0.88      0.88      0.86      2725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report= classification_report(y_test,clf.predict(X_test))\n",
    "print('Classification Report:\\n',report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-f6df10aab053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#import gensim_downloader as api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mwv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# to import smaller size of pre_trained model I will use the following code(only 200k most common words from Google News corpus loaded):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#word2vec_path= '../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/gensim-data\\word2vec-google-news-300\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word2vec-google-news-300.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[1;31m# TODO use frombuffer or something similar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                 \u001b[0madd_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36madd_word\u001b[1;34m(word, weights)\u001b[0m\n\u001b[0;32m    364\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vocabulary file is incomplete: '%s' is missing\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1.Word2Vec:\n",
    "import gensim\n",
    "# we are going to use pre_trained model from 'Google News dataset'\n",
    "\n",
    "#import gensim_downloader as api\n",
    "import gensim.downloader as api\n",
    "wv=api.load('word2vec-google-news-300')\n",
    "# to import smaller size of pre_trained model I will use the following code(only 200k most common words from Google News corpus loaded):\n",
    "#word2vec_path= '../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin' \n",
    "#word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=200000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain vectors for terms the model is familiar with:\n",
    "vec_disaster=wv['disaster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23339844, -0.00067139, -0.05004883,  0.18945312,  0.18359375,\n",
       "        0.21582031, -0.10400391, -0.33398438,  0.484375  , -0.12792969,\n",
       "        0.00656128, -0.19726562,  0.02355957,  0.03515625,  0.02807617,\n",
       "        0.39257812,  0.02526855,  0.08007812,  0.08056641, -0.375     ,\n",
       "        0.12207031,  0.01928711,  0.02612305, -0.2109375 ,  0.26367188,\n",
       "        0.13867188,  0.02368164, -0.36914062,  0.26953125, -0.12792969,\n",
       "       -0.21875   ,  0.11230469, -0.06884766, -0.09375   , -0.18066406,\n",
       "       -0.11914062, -0.11279297, -0.15429688, -0.01782227,  0.22070312,\n",
       "        0.08984375, -0.07910156,  0.24511719,  0.03369141,  0.17871094,\n",
       "       -0.27929688, -0.07177734,  0.30273438,  0.01495361,  0.37304688,\n",
       "        0.03149414, -0.07910156,  0.0255127 ,  0.05737305, -0.21582031,\n",
       "       -0.01672363, -0.2109375 , -0.06298828, -0.09960938,  0.012146  ,\n",
       "       -0.18945312,  0.2109375 ,  0.03393555, -0.30273438, -0.01977539,\n",
       "       -0.20898438,  0.05639648,  0.08740234,  0.11572266, -0.28320312,\n",
       "       -0.2109375 , -0.35351562, -0.00145721,  0.08740234, -0.24511719,\n",
       "       -0.13769531, -0.00119781, -0.06738281, -0.17578125, -0.10693359,\n",
       "        0.21582031,  0.25390625,  0.078125  , -0.03979492, -0.02062988,\n",
       "        0.01031494, -0.14648438, -0.10205078, -0.01141357, -0.0300293 ,\n",
       "        0.08398438, -0.02954102,  0.08886719,  0.10107422, -0.23046875,\n",
       "        0.24414062, -0.05102539,  0.08349609, -0.06494141,  0.20703125,\n",
       "        0.10742188, -0.09521484,  0.07324219,  0.15625   , -0.18359375,\n",
       "        0.03222656,  0.00619507, -0.17773438,  0.06396484,  0.34179688,\n",
       "       -0.15332031,  0.01721191, -0.03515625,  0.078125  ,  0.09570312,\n",
       "        0.10058594,  0.10302734,  0.09082031, -0.06298828,  0.21582031,\n",
       "       -0.19921875, -0.00325012,  0.03271484,  0.47460938, -0.25585938,\n",
       "        0.0300293 ,  0.0625    , -0.08447266,  0.35742188, -0.04418945,\n",
       "       -0.328125  ,  0.13183594, -0.23828125, -0.00057983,  0.04882812,\n",
       "       -0.19335938, -0.06933594, -0.04956055, -0.15136719,  0.08105469,\n",
       "        0.08056641, -0.09326172, -0.04736328,  0.10253906,  0.29882812,\n",
       "       -0.484375  , -0.40234375,  0.08154297, -0.21875   , -0.09033203,\n",
       "       -0.07421875, -0.17089844, -0.1640625 ,  0.12988281, -0.22167969,\n",
       "       -0.21679688,  0.08984375,  0.07910156,  0.11230469, -0.265625  ,\n",
       "        0.14160156, -0.01519775, -0.01647949,  0.02978516, -0.02636719,\n",
       "       -0.09521484,  0.20117188,  0.03393555,  0.20117188,  0.06738281,\n",
       "        0.03808594, -0.07324219, -0.14257812, -0.06738281,  0.19921875,\n",
       "        0.03027344,  0.28320312,  0.02539062, -0.03112793,  0.0456543 ,\n",
       "       -0.13183594, -0.15917969, -0.18945312, -0.44140625,  0.08447266,\n",
       "        0.03088379, -0.21386719, -0.07128906, -0.05322266,  0.12353516,\n",
       "        0.05004883,  0.01953125,  0.10839844, -0.07421875, -0.11035156,\n",
       "        0.15039062,  0.1953125 , -0.22460938, -0.19042969, -0.19628906,\n",
       "        0.09619141,  0.04614258,  0.16015625, -0.19335938, -0.18164062,\n",
       "        0.17578125,  0.11962891, -0.17382812,  0.13867188,  0.3515625 ,\n",
       "       -0.10205078, -0.21875   ,  0.26757812,  0.07128906,  0.01000977,\n",
       "        0.14648438, -0.08886719, -0.30664062, -0.21191406,  0.17871094,\n",
       "       -0.10839844,  0.00878906, -0.09619141,  0.09863281,  0.01855469,\n",
       "       -0.25390625,  0.01391602,  0.16210938,  0.31640625, -0.21679688,\n",
       "        0.14746094,  0.19628906,  0.04003906, -0.31640625, -0.09765625,\n",
       "        0.03125   , -0.06005859, -0.06835938, -0.04931641, -0.2734375 ,\n",
       "        0.22851562,  0.296875  , -0.08154297, -0.26367188,  0.04125977,\n",
       "        0.0324707 ,  0.28320312,  0.02722168, -0.09375   , -0.49023438,\n",
       "       -0.4765625 ,  0.13378906,  0.13574219,  0.03112793,  0.11181641,\n",
       "       -0.13671875, -0.02075195,  0.04125977,  0.01782227,  0.0123291 ,\n",
       "       -0.34960938,  0.19140625,  0.18164062,  0.15429688,  0.03173828,\n",
       "        0.13671875,  0.07519531,  0.0625    , -0.28320312, -0.47460938,\n",
       "       -0.14355469,  0.28515625, -0.05444336,  0.04272461, -0.25390625,\n",
       "        0.31835938,  0.10107422,  0.2578125 , -0.1953125 , -0.01324463,\n",
       "       -0.05371094, -0.00454712,  0.15039062,  0.21289062, -0.0100708 ,\n",
       "        0.03344727,  0.02563477, -0.13378906,  0.11181641,  0.10107422,\n",
       "       -0.09814453, -0.3359375 , -0.25      ,  0.05297852,  0.11230469,\n",
       "        0.00299072, -0.23046875, -0.09570312, -0.05932617,  0.10449219],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim .models.keyedvectors import KeyedVectors\n",
    "word_vectors=KeyedVectors.load_word2vec_format('C:/Users/Sepehr/Desktop/MachineLearning/Data Sets/GoogleNews-vectors-negative300.bin.gz',binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'disaster'\t'thunderstorm'\t0.21\n",
      "'disaster'\t'volcano'\t0.25\n",
      "'disaster'\t'tornado'\t0.42\n",
      "'disaster'\t'drought'\t0.35\n",
      "'disaster'\t'fire'\t0.23\n"
     ]
    }
   ],
   "source": [
    "# Lets see the similiar words to disaster:\n",
    "# To see which type of \n",
    "pairs = [\n",
    "    ('disaster', 'thunderstorm'), \n",
    "    ('disaster', 'volcano'),   \n",
    "    ('disaster', 'tornado'),  \n",
    "    ('disaster', 'drought'),    \n",
    "    ('disaster', 'fire'),\n",
    "]\n",
    "#for w1,w2 in pairs:\n",
    "#    print('The similarity of {} and {} is {}' .format(w1,w2,wv.similarity(w1,w2)))\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, word_vectors.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain vectors for terms the model is familiar with:\n",
    "vec_disaster=word_vectors['disaster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X,y\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, y_train, y_test= train_test_split(df['text'],df['target'],random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim.KeyedVectors.most_similar()\n",
    "Method provides an efficient way to find the nearest neighbors for any given word vector.\n",
    "Positive takes a list of the vectors to be added together, similar to your soccer team example.\n",
    "Similarly, you can use the negative argument for subtraction and to exclude unrelated terms. \n",
    "The argument topn determines how many related terms should be provided as a return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('forest_fires', 0.6209976673126221),\n",
       " ('forests', 0.6181188225746155),\n",
       " ('fires', 0.6173016428947449),\n",
       " ('tinder_dry', 0.5834121108055115),\n",
       " ('wildland_fire', 0.5755112767219543)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['fire', 'forest', 'dry'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tsunami', 0.45853671431541443),\n",
       " ('tsunamis', 0.40876272320747375),\n",
       " ('earthquake', 0.38309282064437866),\n",
       " ('devastating_earthquake', 0.3745158314704895),\n",
       " ('quake', 0.3698951005935669)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['tsunami','ground'],negative=['ocean'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Katrina', 0.7806007862091064),\n",
       " ('Hurricane_Gustav', 0.6598857641220093),\n",
       " ('Hurricane_Rita', 0.651625394821167),\n",
       " ('hurricanes_Katrina', 0.6423860788345337),\n",
       " ('Hurricane_Ike', 0.6333887577056885)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['Hurricane_Katrina','Louisiana' ],negative=['Alabama'],topn=5)\n",
    "\n",
    "# Gustav, Rita are the hurricane happend in Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Manchester', 0.5610477328300476),\n",
       " ('Arsenal', 0.5573516488075256),\n",
       " ('Leeds', 0.5344394445419312),\n",
       " ('Stamford_Bridge', 0.5229190587997437),\n",
       " ('Old_Trafford', 0.5213025808334351)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for fun, I compared Barcelona FC in Spain with what in England:\n",
    "word_vectors.most_similar(positive=['Barcelona','England'],negative=['Spain'],topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim.KeyedVectors.doesnt_match()\n",
    "To determine the most unrelated term of the list, the method returns the term with the highest distance to all other list terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sea'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.doesnt_match('natural disaster sea'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim.KeyedVectors.similarity()\n",
    "The gensim library also allows you to calculate the similarity between two terms. If you want to compare two words and determine their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5394472"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('disaster', 'earthquake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73672867"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('Hurricane_Katrina', 'Hurricane_Ike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555179"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('Hurricane', 'Ike')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a vector representation on disaster tweets:\n",
    "The gensimword2vec model expects a list of sentences, where each sentence\n",
    "is broken up into tokens.\n",
    "To create vector repersentation of the words, word2vec needs a list of lists of tokens.\n",
    "\n",
    "Detector Morse, by Kyle Gorman and OHSU on pypi and at https://github.com/cslu-nlp/DetectorMorse, is a sentence segmenter with state-of-the-art performance (98%) and has been pretrained on sentences from years of text in the Wall Street Journal. So if your corpus includes language similar to that in the WSJ, Detector Morse is likely to give you the highest accuracy currently possible. You can also retrain Detector Morse on your own dataset if you have a large set of sentences from your domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters to control word2vec model training:\n",
    "num_features = 300\n",
    "min_word_count = 3\n",
    "num_workers = 2\n",
    "\n",
    "or to have dynamic multiprocessing do the following:\n",
    "import multiprocessing\n",
    "num_workers=multiprocessing.cpu_count()\n",
    "\n",
    "window_size = 6\n",
    "subsampling = 1e-3 or between 1e-5 and 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use 'lemmatized_word_with_pos'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# to have dynamic multiprocessing do the following:\n",
    "num_features = 300\n",
    "min_word_count = 3\n",
    "\n",
    "import multiprocessing\n",
    "num_workers=multiprocessing.cpu_count()\n",
    "\n",
    "window_size = 6\n",
    "subsampling = 1e-1\n",
    "model=Word2Vec(df['lemmatized_word_with_pos'],\n",
    "               workers=num_workers,\n",
    "               size=num_features,\n",
    "               min_count=min_word_count,\n",
    "               window=window_size,\n",
    "               sample=subsampling\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the model:\n",
    "\n",
    "The init_sims method will freeze the model, storing the weights of the hidden layer and discarding the output weights that predict word co-ocurrences. The output weights aren’t part of the vector used for most Word2vec applications. But the model cannot be trained further once the weights of the output layer have been discarded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading a model:\n",
    "You can save the trained model with the following command and preserve it for\n",
    "later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='my_domain_specific_word2vec_model'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('near', 0.9977343082427979),\n",
       " ('earthquake', 0.9973839521408081),\n",
       " ('seismic', 0.9959284663200378),\n",
       " ('secretary', 0.9944243431091309),\n",
       " ('enormous', 0.9939168691635132)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model_name='my_domain_specific_word2vec_model'\n",
    "model= Word2Vec.load(model_name)\n",
    "model.most_similar('disaster', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('near', 0.9977343082427979),\n",
       " ('earthquake', 0.9973839521408081),\n",
       " ('seismic', 0.9959284663200378),\n",
       " ('secretary', 0.9944243431091309),\n",
       " ('enormous', 0.9939168691635132),\n",
       " ('explosion', 0.9939069747924805),\n",
       " ('temperature', 0.9937769770622253),\n",
       " ('powerful', 0.993468165397644),\n",
       " ('section', 0.9928772449493408),\n",
       " ('64', 0.9927276372909546)]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sample is very small and the result confirms the model is not good. Therefore, I will use google news model. \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model_name='my_domain_specific_word2vec_model'\n",
    "model= Word2Vec.load(model_name)\n",
    "\n",
    "model.similarity('disaster', 'earthquake')\n",
    "model.most_similar('disaster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "fasttext_path = 'C:/Users/Sepehr/Desktop/MachineLearning/Data Sets/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec'\n",
    "fasttext_model = gensim.models.KeyedVectors.load_word2vec_format(fasttext_path, binary=False, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('catastrophe', 0.8263336420059204),\n",
       " ('disasters', 0.8218443393707275),\n",
       " ('calamity', 0.7924492955207825),\n",
       " ('Disaster', 0.7637386918067932),\n",
       " ('tragedy', 0.7395144104957581),\n",
       " ('catastrophes', 0.6992127299308777),\n",
       " ('calamities', 0.6484513282775879),\n",
       " ('devastation', 0.6451743841171265),\n",
       " ('crisis', 0.6386182308197021),\n",
       " ('accident', 0.6301858425140381)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.similarity('disaster', 'earthquake')\n",
    "fasttext_model.most_similar('disaster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<8200x18286 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 121848 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train_vect = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train_vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 18286\n",
      "Every 2000th feature:\n",
      "['00', 'becaus', 'cosmopolitan', 'extinct', 'hurst', 'make', 'pauli', 'runners', 'tamara', 'write']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\n",
    "\n",
    "# As can be seen in below, 18268 features are a lot. we should find a way to decrease the number of features. \n",
    "# But before let see how is the performance of our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation AUC: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Lets check LogisticRegression on this data set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(), X_train_vect, y_train, cv=5,scoring='roc_auc')\n",
    "\n",
    "print(\"Mean cross-validation AUC: {:.2f}\".format(np.mean(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets decrese the number of features with lemmatization:\n",
    "#      Do not run this code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "df['text_lemmatized'] = df['text'].apply(lemmatize_text)\n",
    "#vect = CountVectorizer().fit(text_train)\n",
    "#X_train = vect.transform(text_train)\n",
    "df['text_lemmatized'].iloc[:20]\n",
    "\n",
    "#print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number iof features in countVectorizor:  3171\n",
      "CountVectorizer AUC:  0.767954623352087\n",
      "CountVectorizer f1:  0.6625766871165644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "print('The number iof features in countVectorizor: ',len(vect.get_feature_names()))\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "\n",
    "print('CountVectorizer AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('CountVectorizer f1: ', f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features in tf-idf:  3171\n",
      "tf-idf AUC:  0.6839081580807531\n",
      "tf-idf f1:  0.527536231884058\n"
     ]
    }
   ],
   "source": [
    "# Lets use tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score \n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "# X_train=df['text_lemmatized']\n",
    "vect = TfidfVectorizer(min_df=5).fit(text_train)\n",
    "print('The number of features in tf-idf: ',len(vect.get_feature_names()))\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "\n",
    "print('tf-idf AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('tf-idf f1: ', f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number iof features in countVectorizor:  5470\n",
      "CountVectorizer AUC:  0.7565863110249129\n",
      "tf-idf f1:  0.6427688504326329\n"
     ]
    }
   ],
   "source": [
    "# n-gram- count\n",
    "# lets try to see if the combination of words can improve the performance of the model\n",
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(text_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "print('The number iof features in countVectorizor: ',len(vect.get_feature_names()))\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "print('CountVectorizer AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('tf-idf f1: ', f1_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number iof features in TfidfVectorizor:  5470\n",
      "tf-idf AUC:  0.6813254640350217\n",
      "tf-idf f1:  0.526002971768202\n"
     ]
    }
   ],
   "source": [
    "# n-gram- tf-idf\n",
    "# lets try to see if the combination of words can improve the performance of the model\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(text_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(text_train)\n",
    "print('The number iof features in TfidfVectorizor: ',len(vect.get_feature_names()))\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(text_test))\n",
    "print('tf-idf AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('tf-idf f1: ', f1_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Sepehr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('clf',\n",
       "                                        LogisticRegression(C=1.0,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='lbfgs',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'clf__C': [0.01, 0.1, 1, 10, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets try to improve the performance by tunning hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe=Pipeline([('clf',LogisticRegression())])\n",
    "param_grid = {'clf__C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
